<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Junction Tree Algorithm</title>
  <meta name="description" content="Lecture notes for Stanford cs228.">


  <link rel="stylesheet" href="/cs228-notes/css/tufte.css">	
  

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75587219-1', 'auto');
  ga('send', 'pageview');

  </script>

  <link rel="canonical" href="http://localhost:4000/cs228-notes/inference/jt/">
  <link rel="alternate" type="application/rss+xml" title="Probabilistic graphical modeling course" href="http://localhost:4000/cs228-notes/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
        <a href="/cs228-notes/">Contents</a>
	<a href="http://cs.stanford.edu/~ermon/cs228/index.html">Class</a>
	<a href="http://github.com/ermongroup/cs228-notes">Github</a>
	</nav>
</header>

    <article class="group">
      <h1>Junction tree algorithm</h1>
<p class="subtitle"></p>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      Macros: {
        e: "\\epsilon",
        xti: "x^{(i)}",
        yti: "y^{(i)}",
        bfy: "{\\bf y}",
        bfx: "{\\bf x}",
        bfg: "{\\bf g}",
        bfbeta: "{\\bf \\beta}",
        tp: "\\tilde p",
        pt: "p_\\theta",
        Exp: "{\\mathbb{E}}",
        Ind: "{\\mathbb{I}}",
        KL: "{\\mathbb{KL}}",
        Dc: "{\\mathcal{D}}",
        Tc: "{\\mathcal{T}}",
        Xc: "{\\mathcal{X}}",
        note: ["\\textcolor{blue}{[NOTE: #1]}",1]
      }
    }
  });
</script>


<p>We have seen how the variable elimination (VE) algorithm can answer marginal queries of the form <script type="math/tex">P(Y \mid E = e)</script> for both directed and undirected networks.</p>

<p>However, this algorithm still has an important shortcoming: if we want to ask the model for another query, e.g. <script type="math/tex">P(Y_2 \mid E_2 = e_2)</script>, we need to restart the algorithm from scratch. This is very wasteful and computationally burdensome.</p>

<p>Fortunately, it turns out that this problem is also easily avoidable. When computing marginals, VE produces many intermediate factors <script type="math/tex">\tau</script> as a side-product of the main computation; these factors turn out to be the same as the ones that we need to answer other marginal queries. By caching them after a first run of VE, we can easily answer new marginal queries at essentially no additional cost.</p>

<p>The end result of this chapter will be a new technique called the Junction Tree (JT) algorithm; this algorithm will first execute two runs of the VE algorithm to initialize a particular data structure holding a set of pre-computed factors. Once the structure is initialized, it will be used to answer marginal queries in <script type="math/tex">O(1)</script> time.</p>

<p>We will introduce two variants of this algorithm: belief propagation, and then the full junction tree method. The first one will apply to tree-structured graphs, while the other will be applicable to general networks.</p>

<h2 id="belief-propagation">Belief propagation</h2>

<h3 id="variable-elimination-as-message-passing">Variable elimination as message passing</h3>

<p>First, consider what happens if we run the VE algorithm on a tree in order to compute a marginal <script type="math/tex">p(x_i)</script>. We can easily find an optimal ordering for this problem by rooting the tree at <script type="math/tex">x_i</script> and iterating through the nodes in post-order<label for="1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="1" class="margin-toggle" /><span class="sidenote">A postorder traversal of a rooted tree is one that starts from the leaves and goes up the tree such that a node is always visited after all of its children. The root is visited last. </span>.</p>

<p>This ordering is optimal because the largest clique that formed during VE will be of size 2. At each step, we will eliminate <script type="math/tex">x_j</script>; this will involve computing the factor <script type="math/tex">\tau_k(x_k) = \sum_{x_j} \phi(x_k, x_j) \tau_j(x_j)</script>, where <script type="math/tex">x_k</script> is the parent of <script type="math/tex">x_j</script> in the tree. At a later step, <script type="math/tex">x_k</script> will be eliminated, and <script type="math/tex">\tau_k(x_k)</script> will be passed up the tree to the parent <script type="math/tex">x_l</script> of <script type="math/tex">x_k</script> in order to be multiplied by the factor <script type="math/tex">\phi(x_l, x_k)</script> before being marginalized out. We can visualize this transfer of information using arrows on a tree.
<label for="mp1" class="margin-toggle">⊕</label><input type="checkbox" id="mp1" class="margin-toggle" /><span class="marginnote"><img class="fullwidth" src="/cs228-notes/assets/img/mp1.png" /><br />Message passing order when using VE to compute <script type="math/tex">p(x_3)</script> on a small tree.</span></p>

<p>In a sense, when <script type="math/tex">x_k</script> is marginalized out, it receives all the signal from variables underneath it from the tree. Because of the tree structure (variables affect each other only through their direct neighbors), this signal can be completely summarized in a factor <script type="math/tex">\tau(x_j)</script>. Thus, it makes sense to think of the <script type="math/tex">\tau(x_j)</script> as a message that <script type="math/tex">x_j</script> sends to <script type="math/tex">x_k</script> to summarize all it knows about its children variables.</p>

<p>At the end of the VE run, <script type="math/tex">x_i</script> receives messages from all of its immediate children, marginalizes them out, and we obtain the final marginal.</p>

<p>Now suppose that after computing <script type="math/tex">p(x_i)</script>, we wanted to compute <script type="math/tex">p(x_k)</script> as well. We would again run VE elimination with <script type="math/tex">x_k</script> as the root. We would again wait until <script type="math/tex">x_k</script> receives all messages from its children. The key insight here is that the messages <script type="math/tex">x_k</script> will receive from <script type="math/tex">x_j</script> will be the same as when <script type="math/tex">x_i</script> was the root<label for="1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="1" class="margin-toggle" /><span class="sidenote">Another reason why this is true is because there is only a single path connecting two nodes in the tree. </span>. Thus, if we store the intermediary messages of the VE algorithm, we can quickly recompute other marginals as well.</p>

<h3 id="a-message-passing-algorithm">A message-passing algorithm</h3>

<p>A key question here is how exactly do we compute all the message we need. Notice for example, that the messages to <script type="math/tex">x_k</script> from the side of <script type="math/tex">x_i</script> will need to be recomputed.</p>

<p>The answer is very simple: a node <script type="math/tex">x_i</script> sends a message to a neighbor <script type="math/tex">x_j</script> whenever it has received messages from all nodes besides <script type="math/tex">x_j</script>. It’s a fun exercise to the reader to show that there will always be a node with a message to send, unless all the messages have been sent out. This will happen after precisely <span>​<script type="math/tex">2|E|</script></span> steps, since each edge can receive messages only twice: once from <script type="math/tex">x_i \to x_j</script>, and once more in the opposite direction.</p>

<p>Finally, this algorithm will be correct because our messages are defined as the intermediate factors in the VE algorithm.</p>

<h3 id="sum-product-message-passing">Sum-product message passing</h3>

<p>We are now ready to formally define the belief propagation algorithm. This algorithm will have two variants, the first of which is called sum-product message passing. This algorithm is defined as follows: while there is a node <script type="math/tex">x_i</script> ready to transmit to <script type="math/tex">x_j</script>, send the message</p>
<div class="mathblock"><script type="math/tex; mode=display">
m_{i\to j}(x_j) = \sum_{x_i} \phi(x_i) \phi(x_i,x_j) \prod_{\ell \in N(i) \setminus j} m_{\ell \to i}(x_i).
</script></div>

<p>Again, observe that this message is precisely the factor <script type="math/tex">\tau</script> that <script type="math/tex">x_i</script> would transmit to <script type="math/tex">x_j</script> during a round of variable elimination with the goal of computing <script type="math/tex">p(x_j)</script>.</p>

<p>Because of this observation, after we have computed all messages, we may answer any marginal query over <script type="math/tex">x_i</script> in constant time using the equation</p>
<div class="mathblock"><script type="math/tex; mode=display">
p(x_i) = \prod_{\ell \in N(i)} m_{\ell \to i}(x_i).
</script></div>

<h3 id="max-product-message-passing">Max-product message passing</h3>

<p>So far, we have said very little about the second type of inference we are interested to perform, which are MAP queries</p>
<div class="mathblock"><script type="math/tex; mode=display">
\max_{x_1, \dots, x_n} p(x_1,...,x_n).
</script></div>

<p>The framework we have introduced for marginal queries now lets us easily perform MAP queries as well. The key observation to make, is that we can decompose the problem of MAP inference in exactly the same way as we decomposed the marginal inference problem by replacing sums with maxes.</p>

<p>For example, we may compute the partition function of a chain MRF as follows:</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align*}
Z
& = \sum_{x_1} \cdots \sum_{x_n} \phi(x_1) \prod_{i=2}^n \phi(x_i , x_{i-1}) \\
& = \sum_{x_n} \sum_{x_{n-1}} \phi(x_n , x_{n-1}) \sum_{x_{n-2}} \phi(x_{n-1} , x_{n-2}) \cdots \sum_{x_1} \phi(x_2 , x_1) \phi(x_1) .
\end{align*}
</script></div>

<p>To compute the mode <script type="math/tex">\tp^*</script> of <script type="math/tex">\tp(x_1,...,x_n)</script>, we simply replace sums with maxes, i.e.</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align*}
\tp^*
& = \max_{x_1} \cdots \max_{x_n} \phi(x_1) \prod_{i=2}^n \phi(x_i , x_{i-1}) \\
& = \max_{x_n} \max_{x_{n-1}} \phi(x_n , x_{n-1}) \max_{x_{n-2}} \phi(x_{n-1} , x_{n-2}) \cdots \max_{x_1} \phi(x_2 , x_1) \phi(x_1) .
\end{align*}
</script></div>

<p>The key property that makes this work is the distributivity of both the sum and the max operator over products. Since both problems are essentially equivalent (after swapping the corresponding operators), we may reuse all of the machinery developed for marginal inference and apply it directly to MAP inference.</p>

<p>There is a small caveat in that we often want not just the mode of a distribution, but also its most probable assignment. This problem can be easily solved by keeping <em>back-pointers</em> during the optimization procedure. For instance, in the above example, we would keep a backpointer to the best assignment to <script type="math/tex">x_1</script> given each assignment to <script type="math/tex">x_2</script>, a pointer to the best assignment to <script type="math/tex">x_2</script> given each assignment to <script type="math/tex">x_3</script>, and so on.</p>

<h2 id="junction-tree-algorithm">Junction tree algorithm</h2>

<p>So far, our discussion assumed that the graph is a tree. What if that is not the case? Inference in that case will not be tractable; however, we may try to massage the graph to its most tree-like form, and then run message passing on this graph.</p>

<p>At a high-level the junction tree algorithm will try to achieve this by partitioning the graph into clusters of variables; internally, the variables within clusters could be highly coupled; however, interactions <em>among</em> clusters will have a tree structure, i.e. a cluster will be only directly influenced by its neighbors in the tree. This will lead to tractable global solutions if some local (cluster-level) problems can be solved exactly.</p>

<h3 id="an-illustrative-example">An illustrative example</h3>

<p>Before we define the full algorithm, let us first start with an example, like we did for the variable elimination algorithm.</p>

<p>Suppose that we are performing marginal inference and that we are given an MRF of the form</p>
<div class="mathblock"><script type="math/tex; mode=display">
p(x_1,..,x_n) = \frac{1}{Z} \prod_{c \in C} \phi_c(x_c),
</script></div>

<p>Crucially, we will assume that the cliques <script type="math/tex">c</script> have a form of path structure, meaning that we can find an ordering <script type="math/tex">x_c^{(1)}, ..., x_c^{(n)}</script> with the property that if <script type="math/tex">x_i \in x_c^{(j)}</script> and <script type="math/tex">x_i \in x_c^{(k)}</script> for some variable <script type="math/tex">x_i</script> then <script type="math/tex">x_i \in x_c^{(\ell)}</script> for all <script type="math/tex">x_c^{(\ell)}</script> on the path between <script type="math/tex">x_c^{(j)}</script> and <script type="math/tex">x_c^{(k)}</script>. We refer to this assumption as the <em>running intersection</em> (RIP) property.</p>
<figure><figcaption>A chain MRF whose cliques are organized into a chain structure. Round nodes represent cliques and the variables in their scope; rectangular nodes indicate sepsets, which are variables forming the intersection of the scopes of two neighboring cliques</figcaption><img src="/cs228-notes/assets/img/junctionpath.png" /></figure>

<p>Suppose that we are interested in computing the marginal probability <script type="math/tex">p(x_1)</script> in the above example. Given our assumptions, we may again use a form of variable elimination to 
“push in” certain variables deeper into the product of cluster potentials:</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align*}
\phi(x_1) \sum_{x_2} \phi(x_1,x_2) \sum_{x_3} \phi(x_1,x_2,x_3) \sum_{x_5} \phi(x_2,x_3,x_5) \sum_{x_6} \phi(x_2,x_5,x_6).
\end{align*}
</script></div>

<p>We first sum over <script type="math/tex">x_6</script>, which creates a factor <script type="math/tex">\tau(x_2, x_3, x_5) = \phi(x_2,x_3,x_5) \sum_{x_6} \phi(x_2,x_5,x_6)</script>. Then, <script type="math/tex">x_5</script> gets eliminated, and so on. At each step, each cluster marginalizes out the variables that are not in the scope of its neighbor. This marginalization can also be interpreted as computing a message over the variables it shares with the neighbor.</p>

<p>The running intersection property is what enables us to push sums in all the way to the last factor. We may eliminate <script type="math/tex">x_6</script> because we know that only the last cluster will carry this variable: since it is not present in the neighboring cluster, it cannot be anywhere else in the graph without violating the RIP.</p>

<h3 id="junction-trees">Junction trees</h3>

<p>The core idea of the junction tree algorithm is to turn a graph into a tree of clusters that are amenable to the variable elimination algorithm like the above MRF. Then we simply perform message-passing on this tree.</p>

<p>Suppose we have an undirected graphical model <script type="math/tex">G</script> (if the model is directed, we consider its moralized graph).
A junction tree <script type="math/tex">T=(C, E_T)</script> over <script type="math/tex">G = (\Xc, E_G)</script> is a tree whose nodes <script type="math/tex">c \in C</script> are associated with subsets <script type="math/tex">x_c \subseteq \Xc</script> of the graph vertices (i.e. sets of variables); the junction tree must satisfy the following properties:</p>

<ul>
  <li><em>Family preservation</em>: For each factor <script type="math/tex">\phi</script>, there is a cluster <script type="math/tex">c</script> such that <script type="math/tex">\text{Scope}[\phi] \subseteq x_c</script>.</li>
  <li><em>Running intersection</em>: For every pair of clusters <script type="math/tex">c^{(i)}, c^{(j)}</script>, every cluster on the path between <script type="math/tex">c^{(i)}, c^{(j)}</script> contains <script type="math/tex">x_c^{(i)} \cap x_c^{(j)}</script>.</li>
</ul>

<p>Here is an example of an MRF with graph <script type="math/tex">G</script> and junction tree <script type="math/tex">T</script>. MRF potentials are denoted using different colors; circles indicates nodes of the junction trees; rectangular nodes represent <em>sepsets</em>, which are sets of variables shared by neighboring clusters.</p>

<figure><figcaption>An MRF with graph G and its junction tree T.</figcaption><img src="/cs228-notes/assets/img/junctiontree.png" /></figure>
<p><label for="jtt" class="margin-toggle">⊕</label><input type="checkbox" id="jtt" class="margin-toggle" /><span class="marginnote"><img class="fullwidth" src="/cs228-notes/assets/img/jt-over-tree.png" /><br />A junction tree defined over a tree graph. Clusters correspond to edges.</span>
<label for="bjt" class="margin-toggle">⊕</label><input type="checkbox" id="bjt" class="margin-toggle" /><span class="marginnote"><img class="fullwidth" src="/cs228-notes/assets/img/badjunctiontree.png" /><br />Example of an invalid junction tree that does not satisfy the running intersection property.</span></p>

<p>Note that we may always find a trivial junction tree with one node containing all the variables in the original graph. However, such trees are useless because they will not result in efficient marginalization algorithms.</p>

<p>Optimal trees are one that make the clusters as small and modular as possible; unfortunately, it is again NP-hard to find the optimal tree. We will see below some practical ways in which we can find good junction trees</p>

<p>A special case when we <em>can</em> find the optimal junction tree is when <script type="math/tex">G</script> itself is a tree. In that case, we may define a cluster for each edge in the tree. It is not hard to check that the result satisfies the above definition.</p>

<h3 id="the-junction-tree-algorithm">The junction tree algorithm</h3>

<p>Let us now define the junction tree algorithm, and then explain why it works. At a high-level, this algorithm implements a form of message passing on the junction tree, which will be equivalent to variable elimination for the same reasons that BP was equivalent to VE.</p>

<p>More precisely, let us define the potential <script type="math/tex">\psi_c(x_c)</script> of each cluster <script type="math/tex">c</script> as the product of all the factors <script type="math/tex">\phi</script> in <script type="math/tex">G</script> that have been assigned to <script type="math/tex">c</script>. By the family preservation property, this is well-defined, and we may assume that our distribution is in the form</p>
<div class="mathblock"><script type="math/tex; mode=display">
p(x_1,..,x_n) = \frac{1}{Z} \prod_{c \in C} \psi_c(x_c).
</script></div>

<p>Then, at each step of the algorithm, we choose a pair of adjacent clusters <script type="math/tex">c^{(i)}, c^{(j)}</script> in <script type="math/tex">T</script> and compute a message whose scope is the sepset <script type="math/tex">S_{ij}</script> between the two clusters:</p>
<div class="mathblock"><script type="math/tex; mode=display">
m_{i\to j}(S_{ij}) = \sum_{x_c \backslash S_{ij}} \psi(x_c) \prod_{\ell \in N(i) \backslash j} m_{\ell \to i}(S_{\ell i}).
</script></div>

<p>We choose <script type="math/tex">c^{(i)}, c^{(j)}</script> only if <script type="math/tex">c^{(i)}</script> has received messages from all of its neighbors except <script type="math/tex">c^{(j)}</script>. Just as in belief propagation, this procedure will terminate in exactly <span>​<script type="math/tex">2|E_T|</script></span> steps. After it terminates, we will define the belief of each cluster based on all the messages that it receives</p>
<div class="mathblock"><script type="math/tex; mode=display">
\beta_c(x_c) = \psi(x_c) \prod_{\ell \in N(i)} m_{\ell \to i}(S_{\ell i}).
</script></div>

<p>These updates are often referred to as <em>Shafer-Shenoy</em>. After all the messages have been passed, beliefs will be proportional to the marginal probabilities over their scopes, i.e. <script type="math/tex">\beta_c(x_c) \propto p(x_c)</script>. We may answer queries of the form <script type="math/tex">\tp(x)</script> for <script type="math/tex">x \in x_c</script> by marginalizing out the variable in its belief<label for="1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="1" class="margin-toggle" /><span class="sidenote">Readers familiar with combinatorial optimization will recognize this as a special case of dynamic programming on a tree decomposition of a graph with bounded treewidth. </span></p>
<div class="mathblock"><script type="math/tex; mode=display">
\tp(x) \propto \sum_{x_c \backslash x} \beta_c(x_c) .
</script></div>
<p>To get the actual (normalized) probability, we divide by the partition function <script type="math/tex">Z</script> which is computed by summing all the beliefs in a cluster, <script type="math/tex">Z = \sum_{x_c} \beta_c(x_c)</script>.</p>

<p>Note that this algorithm makes it obvious why we want small clusters: the running time will be exponential in the size of the largest cluster (if only because we may need to marginalize out variables from the cluster, which often must be done using brute force). This is why a junction tree of a single node containing all the variables is not useful: it amounts to performing full brute-force marginalization.</p>

<h3 id="variable-elimination-over-a-junction-tree">Variable elimination over a junction tree</h3>

<p>Why does this method work? First, let us convince ourselves that running variable elimination with a certain ordering is equivalent to performing message passing on the junction tree; then, we will see that the junction tree algorithm is just a way of precomputing these messages and using them to answer queries.</p>

<p>Suppose we are performing variable elimination to compute <script type="math/tex">\tp(x')</script> for some variable <script type="math/tex">x'</script>, where <script type="math/tex">\tp = \prod_{c \in C} \psi_c</script>. Let <script type="math/tex">c^{(i)}</script> be a cluster containing <script type="math/tex">x'</script>; we will perform VE with the ordering given by the structure of the tree rooted at <script type="math/tex">c^{(i)}</script>. In the example below, say that we choose to eliminate the <script type="math/tex">b</script> variable, and we set <script type="math/tex">(a,b,c)</script> as the root cluster.</p>
<figure><figcaption>An MRF with graph G and its junction tree T.</figcaption><img src="/cs228-notes/assets/img/junctiontree.png" /></figure>

<p>First, we pick a set of variables <script type="math/tex">x_{-k}</script> in a leaf <script type="math/tex">c^{(j)}</script> of <script type="math/tex">T</script> that does not appear in the sepset <script type="math/tex">S_{kj}</script> between <script type="math/tex">c^{(j)}</script> and its parent <script type="math/tex">c^{(k)}</script> (if there is no such variable, we may multiply <script type="math/tex">\psi(x_c^{(j)})</script> and <script type="math/tex">\psi(x_c^{(k)})</script> into a new factor with a scope not larger than that of the initial factors). In our example, we may pick the variable <script type="math/tex">f</script> in the factor <script type="math/tex">(b,e,f)</script>.</p>

<p>Then we marginalize out <script type="math/tex">x_{-k}</script> to obtain a factor <script type="math/tex">m_{j \to k}(S_{ij})</script>. We multiply <script type="math/tex">m_{j \to k}(S_{ij})</script> with <script type="math/tex">\psi(x_c^{(k)})</script> to obtain a new factor <script type="math/tex">\tau(x_c^{(k)})</script>. Doing so, we have effectively eliminated the factor <script type="math/tex">\psi(x_c^{(j)})</script> and the unique variables it contained. In the running example, we may sum out <script type="math/tex">f</script> and the resulting factor over <script type="math/tex">(b, e)</script> may be folded into <script type="math/tex">(b,c,e)</script>.</p>

<p>Note that the messages computed in this case are exactly the same as those of JT. In particular, when <script type="math/tex">c^{(k)}</script> is ready to send its message, it will have been multiplied by <script type="math/tex">m_{\ell \to k}(S_{ij})</script> from all neighbors except its parent, which is exactly how JT sends its message.</p>

<p>Repeating this procedure eventually produces a single factor <script type="math/tex">\beta(x_c^{(i)})</script>, which is our final belief. Since VE implements the messages of the JT algorithm, <script type="math/tex">\beta(x_c^{(i)})</script> will correspond to the JT belief. Assuming we have convinced ourselves in the previous section that VE works, we know that this belief will be valid.</p>

<p>Formally, we may prove correctness of the JT algorithm through an induction argument on the number of factors <script type="math/tex">\psi</script>; we will leave this as an exercise to the reader. The key property that makes this argument possible is the RIP; it assures us that it’s safe to eliminate a variable from a leaf cluster that is not found in that cluster’s sepset; by the RIP, it cannot occur anywhere except that one cluster.</p>

<p>The important thing to note is that if we now set <script type="math/tex">c^{(k)}</script> to be the root of the tree (e.g. if we set <script type="math/tex">(b,c,e)</script> to be the root), the message it will receive from <script type="math/tex">c^{(j)}</script> (or from <script type="math/tex">(b,e,f)</script> in our example) will not change. Hence, the caching approach we used for the belief propagation algorithm extends immediately to junction trees; the algorithm we formally defined above implements this caching.</p>

<h3 id="finding-a-good-junction-tree">Finding a good junction tree</h3>

<p>The last topic that we need to address is the question of constructing good junction trees.</p>

<ul>
  <li><em>By hand</em>: Typically, our models will have a very regular structure, for which there will be an obvious solution. For example, very often our model is a grid, in which case clusters will be associated with pairs of adjacent rows (or columns) in the grid.</li>
  <li><em>Using variable elimination</em>: One can show that running the VE elimination algorithm implicitly generates a junction tree over the variables. Thus it is possible to use the heuristics we previously discussed to define this ordering.</li>
</ul>

<h2 id="loopy-belief-propagation">Loopy belief propagation</h2>

<p>As we have seen, the junction tree algorithm has a running time that is potentially exponential in the size of the largest cluster (since we need to marginalize all the cluster’s variables). For many graphs, it will be difficult to find a good junction tree, applying the algorithm will not be possible. In other cases, we may not need the exact solution that the junction tree algorithm provides; we may be satisfied with a quick approximate solution instead.</p>

<p>Loopy belief propagation (LBP) is another technique for performing inference on complex (non-tree structure) graphs. Unlike the junction tree algorithm, which attempted to efficiently find the exact solution, LBP will form our first example of an approximate inference algorithm.</p>

<h3 id="definition-for-pairwise-models">Definition for pairwise models</h3>

<p>Suppose that we a given an MRF with pairwise potentials<label for="1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="1" class="margin-toggle" /><span class="sidenote">Arbitrary potentials can be handled using an algorithm called LBP on <em>factor graphs</em>. We will include this material at some point in the future </span>.
The main idea of LBP is to disregard loops in the graph and perform message passing anyway. In other words, given an ordering on the edges, at each time <script type="math/tex">t</script> we iterate over a pair of adjacent variables <script type="math/tex">x_i, x_j</script> in that order and simply perform the update</p>
<div class="mathblock"><script type="math/tex; mode=display">
m^{t+1}_{i\to j}(x_j) = \sum_{x_i} \phi(x_i) \phi(x_i,x_j) \prod_{\ell \in N(i) \setminus j} m^{t}_{\ell \to i}(x_i).
</script></div>

<p>We keep performing these updates for a fixed number of steps or until convergence (the messages don’t change). Messages are typically initialized uniformly.</p>

<h3 id="properties">Properties</h3>

<p>This heuristic approach often works surprisingly well in practice.
<label for="mp1" class="margin-toggle">⊕</label><input type="checkbox" id="mp1" class="margin-toggle" /><span class="marginnote"><img class="fullwidth" src="/cs228-notes/assets/img/lbp-performance.png" /><br />Marginals obtained via LBP compared to true marginals obtained from the JT algorithm on an intensive care monitoring task. Results are close to the diagonal, hence very similar.</span>
In general, however, it may not converges and its analysis is still an area of active research. We know for example that it provably converges on trees and on graphs with at most one cycle. If the method does converge, its beliefs may not necessarily equal the true marginals, although very often in practice they will be close.</p>

<p>We will return to this algorithm later in the course and try to explain it as a special case of <em>variational inference</em> algorithms.</p>

<p><br /></p>

<table>
  <tbody>
    <tr>
      <td><a href="../../">Index</a></td>
      <td><a href="../ve">Previous</a></td>
      <td><a href="../map">Next</a></td>
    </tr>
  </tbody>
</table>



    </article>
    <span class="print-footer">Junction Tree Algorithm - Volodymyr Kuleshov</span>
    <footer>
  <hr class="slender">
  <!-- <ul class="footer&#45;links"> -->
  <!--   <li><a href="mailto:hate@spam.net"><span class="icon&#45;mail"></span></a></li>     -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.twitter.com/twitter_handle"><span class="icon-twitter"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//plus.google.com/+googlePlusName"><span class="icon-googleplus"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//github.com/GithubHandle"><span class="icon-github"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.flickr.com/photos/FlickrUserID"><span class="icon-flickr"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="/feed"><span class="icon-feed"></span></a> -->
  <!--     </li> -->
  <!--      -->
  <!-- </ul> -->
<div class="credits">
<!-- <span>&#38;copy; 2017 <!&#45;&#45; &#38;#38;nbsp;&#38;#38;nbsp;VOLODYMYR KULESHOV &#45;&#45;></span></br> <br> -->
<span>Site created with <a href="//jekyllrb.com">Jekyll</a> using the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme</a>. &copy; 2017</span> 
</div>  
</footer>

  </body>
</html>
