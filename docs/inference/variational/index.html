<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Variational inference</title>
  <meta name="description" content="Lecture notes for Stanford cs228.">


  <link rel="stylesheet" href="/cs228-notes/css/tufte.css">	
  

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75587219-1', 'auto');
  ga('send', 'pageview');

  </script>

  <link rel="canonical" href="http://localhost:4000/cs228-notes/inference/variational/">
  <link rel="alternate" type="application/rss+xml" title="Probabilistic graphical modeling course" href="http://localhost:4000/cs228-notes/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
        <a href="/cs228-notes/">Contents</a>
	<a href="http://cs.stanford.edu/~ermon/cs228/index.html">Class</a>
	<a href="http://github.com/ermongroup/cs228-notes">Github</a>
	</nav>
</header>

    <article class="group">
      <h1>Variational inference</h1>
<p class="subtitle"></p>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      Macros: {
        e: "\\epsilon",
        xti: "x^{(i)}",
        yti: "y^{(i)}",
        bfy: "{\\bf y}",
        bfx: "{\\bf x}",
        bfg: "{\\bf g}",
        bfbeta: "{\\bf \\beta}",
        tp: "\\tilde p",
        pt: "p_\\theta",
        Exp: "{\\mathbb{E}}",
        Ind: "{\\mathbb{I}}",
        KL: "{\\mathbb{KL}}",
        Dc: "{\\mathcal{D}}",
        Tc: "{\\mathcal{T}}",
        Xc: "{\\mathcal{X}}",
        note: ["\\textcolor{blue}{[NOTE: #1]}",1]
      }
    }
  });
</script>


<p>In the last chapter, we saw that inference in probabilistic models is often intractable, and we learned about algorithms that provide approximate solutions to the inference problem (e.g. marginal inference) by using subroutines that involve sampling random variables. Most sampling-based inference algorithms are instances of Markov Chain Monte-Carlo (MCMC); two popular MCMC methods are Gibbs sampling and Metropolis-Hastings.</p>

<p>Unfortunately, these sampling-based methods have several important shortcomings.</p>

<ul>
  <li>Although they are guaranteed to find a globally optimal solution given enough time, it is difficult to tell how close they are to a good solution given the finite amount of time that they have in practice.</li>
  <li>In order to quickly reach a good solution, MCMC methods require choosing an appropriate sampling technique (e.g. a good proposal in Metropolis-Hastings). Choosing this technique can be an art in itself.</li>
</ul>

<p>In this chapter, we are going to look at an alternative approach to approximate inference called the <em>variational</em> family of algorithms.</p>

<h2 id="inference-as-optimization">Inference as optimization</h2>

<p>The main idea of variational methods is to cast inference as an optimization problem.</p>

<p>Suppose we are given an intractable probability distribution <script type="math/tex">p</script>. Variational techniques will try to solve an optimization problem over a class of tractable distributions <script type="math/tex">\mathcal{Q}</script> in order to find a <script type="math/tex">q \in \mathcal{Q}</script> that is most similar to <script type="math/tex">p</script>. We will then query <script type="math/tex">q</script> (rather than <script type="math/tex">p</script>) in order to get an approximate solution.</p>

<p>The main differences between sampling and variational techniques are that:</p>

<ul>
  <li>Unlike sampling-based methods, variational approaches will almost never find the globally optimal solution.</li>
  <li>However, we will always know if they have converged. In some cases, we will even have bounds on their accuracy.</li>
  <li>In practice, variational inference methods often scale better and are more amenable to techniques like stochastic gradient optimization, parallelization over multiple processors, and acceleration using GPUs.</li>
</ul>

<p>Although sampling methods were historically invented first (in the 1940’s), variational techniques have been steadily gaining popularity and are currently the more widely used inference technique.</p>

<h2 id="the-kullback-leibler-divergence">The Kullback-Leibler divergence</h2>

<p>To formulate inference as an optimization problem, we need to choose an approximating family <script type="math/tex">\mathcal{Q}</script> and an optimization objective <script type="math/tex">J(q)</script>. This objective needs to capture the similarity between <script type="math/tex">q</script> and <script type="math/tex">p</script>; the field of information theory provides us with a tool for this called the <em>Kullback-Leibler (KL) divergence</em>.</p>

<p>Formally, the KL divergence between two distributions <script type="math/tex">q</script> and <script type="math/tex">p</script> with discrete support is defined as</p>
<div class="mathblock"><script type="math/tex; mode=display">
KL(q||p) = \sum_x q(x) \log \frac{q(x)}{p(x)}.
</script></div>

<p>In information theory, this function is used to measure differences in information contained within two distributions. The KL divergence has the following properties that make it especially useful in our setting:</p>

<ul>
  <li><span>​<script type="math/tex"> KL(q||p) \geq 0 </script></span> for all <span>​<script type="math/tex">q,p</script></span>.</li>
  <li><span>​<script type="math/tex"> KL(q||p) = 0 </script></span> if and only if <span>​<script type="math/tex"> q = p </script></span></li>
</ul>

<p>These can be proven as an exercise. Note however that <span>​<script type="math/tex">KL(q||p) \neq KL(p||q)</script></span>, i.e. the KL divergence is not symmetric. This is why we say that it’s a divergence, but not a distance. We will come back to this distinction shortly.</p>

<h2 id="the-variational-lower-bound">The variational lower bound</h2>

<p>How do we perform variational inference with a KL divergence? First, let’s fix a form for <script type="math/tex">p</script>. We’ll assume that <script type="math/tex">p</script> is a general (discrete, for simplicity) undirected model of the form</p>
<div class="mathblock"><script type="math/tex; mode=display">
p(x_1,..,x_n; \theta) = \frac{\tilde p(x_1,...,x_n ; \theta)}{Z(\theta)} =\frac{1}{Z(\theta)} \prod_{k} \phi_k(x_k; \theta),
</script></div>
<p>where the <script type="math/tex">\phi_k</script> are the factors and <script type="math/tex">Z(\theta)</script> is the normalization constant. This formulation captures virtually all the distributions in which we might want to perform approximate inference, such as marginal distributions of directed models <script type="math/tex">p(x|e) = p(x,e)/p(e)</script> with evidence <script type="math/tex">e</script>.</p>

<p>Given this formulation, optimizing <span>​<script type="math/tex">KL(q||p)</script></span> directly is not possible because of the potentially intractable normalization constant <script type="math/tex">Z(\theta)</script>. In fact, even evaluating <span>​<script type="math/tex">KL(q||p)</script></span> is not possible, because we need to evaluate <script type="math/tex">p</script>.</p>

<p>Instead, we will work with the following objective, which has the same form as the KL divergence, but only involves the unnormalized probability <script type="math/tex">\tilde p = \prod_{k} \phi_k(x_k; \theta)</script>:</p>
<div class="mathblock"><script type="math/tex; mode=display">
J(q) = \sum_x q(x) \log \frac{q(x)}{\tilde p(x)}.
</script></div>

<p>This function is not only tractable, it also has the following important property:</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align*}
J(q) 
& = \sum_x q(x) \log \frac{q(x)}{\tilde p(x)} \\
& = \sum_x q(x) \log \frac{q(x)}{p(x)} - \log Z(\theta) \\
& = KL(q||p) - \log Z(\theta)
\end{align*}
</script></div>
<p>Since <span>​<script type="math/tex"> KL(q||p) \geq 0 </script></span>, we get by rearranging terms that</p>
<div class="mathblock"><script type="math/tex; mode=display">
\log Z(\theta) = KL(q||p) - J(q) \geq -J(q).
</script></div>

<p>Thus, <script type="math/tex">-J(q)</script> is a <em>lower bound</em> on the partition function <script type="math/tex">Z(\theta)</script>. In many cases, <script type="math/tex">Z(\theta)</script> has an interesting interpretation. For example, we may be trying to compute the marginal probability <span>​<script type="math/tex">p(x|D) = p(x,D) / p(D)</script></span> of variables <script type="math/tex">x</script> given observed data <script type="math/tex">D</script> that plays the role of evidence. We assume that <script type="math/tex">p(x,D)</script> is directed. In this case, minimizing <script type="math/tex">J(q)</script> amounts to maximizing a lower bound on the log-likelihood <script type="math/tex">\log p(D)</script> of the observed data.</p>

<p>Because of this property, <script type="math/tex">-J(q)</script> is called the variational lower bound or the evidence lower bound (ELBO); it often written in the form</p>

<div class="mathblock"><script type="math/tex; mode=display">
\log Z(\theta) \geq \mathbb{E}_{q(x)} \left[ \log \tilde p(x) - \log q(x) \right].
</script></div>

<p>Crucially, the difference between <script type="math/tex">\log Z(\theta)</script> and <script type="math/tex">-J(q)</script> is precisely <span>​<script type="math/tex">KL(q||p)</script></span>. Thus, by maximizing the evidence-lower bound, we are minimizing <span>​<script type="math/tex">KL(q||p)</script></span> by “squeezing” it between <script type="math/tex">J(q)</script> and <script type="math/tex">\log Z(\theta)</script>.</p>

<h2 id="on-the-choice-of-kl-divergence">On the choice of KL divergence</h2>

<p>To recap, we have just defined an optimization objective for variational inference (the variational lower bound) and we have shown that maximizing the lower bound leads to minimizing the divergence <span>​<script type="math/tex">KL(q||p)</script></span>.</p>

<p>Recall how we said earlier that <span>​<script type="math/tex">KL(q||p) \neq KL(p||q)</script></span>; both divergences equal zero when <script type="math/tex">q = p</script>, but assign different penalties when <script type="math/tex">q \neq p</script>. This raises the question: why did we choose one over the other and how do they differ?</p>

<p>Perhaps the most important difference is computational: optimizing <span>​<script type="math/tex">KL(q||p)</script></span> involves an expectation with respect to <script type="math/tex">q</script>, while <span>​<script type="math/tex">KL(p||q)</script></span> requires computing expectations with respect to <script type="math/tex">p</script>, which is typically intractable even to evaluate.</p>

<p>However, choosing this particular divergence affects the returned solution when the approximating family <script type="math/tex">\mathcal{Q}</script> does not contain the true <script type="math/tex">p</script>. Observe that <span>​<script type="math/tex">KL(q||p)</script></span> — which is called the I-projection or information projection — is infinite if <script type="math/tex">p(x) == 0</script> and <script type="math/tex">q(x) > 0</script>:</p>
<div class="mathblock"><script type="math/tex; mode=display">
KL(q||p) = \sum_x q(x) \log \frac{q(x)}{p(x)}.
</script></div>
<p>This means that if <script type="math/tex">p(x) = 0</script> we must have <script type="math/tex">q(x) = 0</script>. We say that <span>​<script type="math/tex">KL(q||p)</script></span> is zero-forcing for <script type="math/tex">q</script> and it will typically under-estimate the support of <script type="math/tex">q</script></p>

<p>On the other hand, <span>​<script type="math/tex">KL(p||q)</script></span> — known as the M-projection or the moment projection — is infinite if <script type="math/tex">q(x) == 0</script> and <script type="math/tex">p(x) > 0</script>.
Thus, if <script type="math/tex">p(x) > 0</script> we must have <script type="math/tex">q(x) > 0</script>. We say that <span>​<script type="math/tex">KL(p||q)</script></span> is zero-avoiding for <script type="math/tex">q</script> and it will typically over-estimate the support of <script type="math/tex">q</script>.</p>

<p>The figure below illustrates this phenomenon graphically.</p>

<figure><figcaption>Fitting a unimodal approximating distribution q (red) to a multimodal p (blue). Using KL(p||q) leads to a q that tries to cover both modes (a). However, using KL(q||p) forces q to choose one of the two modes of p (b, c).</figcaption><img src="/cs228-notes/assets/img/kldiv.png" /></figure>

<p>Due to the properties that we just described, we often call <span>​<script type="math/tex">KL(p||q)</script></span> the <em>inclusive</em> KL divergence, while <span>​<script type="math/tex">KL(q||p)</script></span> is the <em>exclusive</em> KL divergence.</p>

<h2 id="mean-field-inference">Mean-field inference</h2>

<p>The next step in our development of variational inference concerns the choice of approximating family <script type="math/tex">\mathcal{Q}</script>. The machine learning literature contains dozens of proposed ways to parametrize this class of distributions; these include exponential families, neural networks, Gaussian processes, latent variable models, and many others types of models.</p>

<p>However, one of the most widely used classes of distributions is simply the set of fully-factored <script type="math/tex">q(x) = q_1(x_1) q_2(x_2) \cdots q_n(x_n)</script>; here each <script type="math/tex">q_i(x_i)</script> is categorical distribution over a one-dimensional discrete variable, which can be described as a one-dimensional table.</p>

<p>This choice of <script type="math/tex">\mathcal{Q}</script> turns out to be easy to optimize over and works surprisingly well. It is perhaps the most popular choice when optimizing the variational bound; variational inference with this choice of <script type="math/tex">\mathcal{Q}</script> is called <em>mean-field</em> inference. It consists in solving the following optimization problem:</p>
<div class="mathblock"><script type="math/tex; mode=display">
\min_{q_1, \cdots, q_n} J(q).
</script></div>

<p>The standard way of performing this optimization problem is via coordinate descent over the <script type="math/tex">q_j</script>: we iterate over <script type="math/tex">j=1,2,...,n</script> and for each <script type="math/tex">j</script> we optimize <span>​<script type="math/tex">KL(q||p)</script></span> over <script type="math/tex">q_j</script> while keeping the other “coordinates” <script type="math/tex">q_{-j} = \prod_{i \neq j} q_i</script> fixed.</p>

<p>Interestingly, the optimization problem for one coordinate has a simple closed form solution:</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align*}
\log q_j(x_j) \gets \mathbb{E}_{q_{-j}} \left[ \log \tilde p (x) \right] + \textrm{const.}
\end{align*}
</script></div>

<p>Notice that both sides of the above equation contain univariate functions of <script type="math/tex">x_j</script>: we are thus replacing <script type="math/tex">q(x_j)</script> with another function of the same form.
The constant term is a normalization constant for the new distribution.</p>

<p>Notice also that on right-hand side, we are taking an expectation of a sum of factors</p>
<div class="mathblock"><script type="math/tex; mode=display">
\log \tilde p (x) = \sum_k \log \phi(x_k)
</script></div>
<p>Of these, only factors belonging to the Markov blanket of <script type="math/tex">x_j</script> are a function of <script type="math/tex">x_j</script> (simply by the definition of the Markov blanket); the rest are constant with respect to <script type="math/tex">x_j</script> and can be pushed into the constant term.</p>

<p>This leaves us with an expectation over a much smaller number of factors; if the Markov blanket of <script type="math/tex">x_j</script> is small (as is often the case), we are able to analytically compute <script type="math/tex">q(x_j)</script>. For example, if the variables are discrete with <script type="math/tex">K</script> possible values, and there are <script type="math/tex">F</script> factors and <script type="math/tex">N</script> variables in the Markov blanket of <script type="math/tex">x_j</script>, then computing the expectation takes <script type="math/tex">O(K F N^K)</script> time: for each value of <script type="math/tex">x_j</script> we sum over all <script type="math/tex">N^K</script> assignments of the <script type="math/tex">N</script> variables, and in each case, we sum over the <script type="math/tex">F</script> factors.</p>

<p>The result of this is a procedure that iteratively fits a fully-factored <script type="math/tex">q(x) = q_1(x_1) q_2(x_2) \cdots q_n(x_n)</script> that approximates <script type="math/tex">p</script> in terms of <span>​<script type="math/tex">KL(q||p)</script></span>. After each step of coordinate descent, we increase the variational lower bound, tightening it around <script type="math/tex">\log Z(\theta)</script>.</p>

<p>In the end, the factors <script type="math/tex">q_j(x_j)</script> will not quite equal the true marginal distributions <script type="math/tex">p(x_j)</script>, but they will often be good enough for many practical purposes, such as determining <script type="math/tex">\max_{x_j} p(x_j)</script>.</p>

<!---
The notes below are incomplete and under construction
## The marginal polytope

A natural question to ask, given what we've seen, is how exactly do the mean-field marginals approximate $$p(x)$$, and what makes them "good enough".

To answer this question, we will take a step back, and define a more general framework for thinking about variational inference. We will then see how mean field and, interestingly, loopy belief propagation (LBP) both fall into this framework and how exactly their approximations differ. As a result, we will also see how LBP is also a form of variational inference, something we have alluded to earlier.

First, recall that we have assumed that the distribution $$p$$ is of the form 
<div class="mathblock"><script type="math/tex; mode=display">
p(x_1,..,x_n; \theta) = \frac{\tilde p(x_1,...,x_n ; \theta)}{Z(\theta)} =\frac{1}{Z(\theta)} \prod_{k} \phi_k(x_k; \theta).
</script></div>
When the variables are discrete, we can further rewrite this as 
<div class="mathblock"><script type="math/tex; mode=display">
p(x_1,..,x_n; \theta) = \frac{\exp(\sum_k \theta^T \psi(x))}{Z(\theta)},
</script></div>
where $$\theta_{k, x_k} = \phi_k(x_k)$$ is now a vector of factor values (one for each variable assignment), and $$\psi_{k, x_k}(x') = \mathbb{I(x'_k = x_k)}($$ is a vector of indicator values for each assignment, given an input $$x'$$. The vector $$\psi_k$$ simply "picks out" the appropriate entries of $$\theta$$.

Given this notation, we can write our objective as
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align*}
-J(x) 
& = \mathbb{E}_{q(x)} \left[ \theta^T \psi(x) - \log q(x) \right] \\
& = \theta^T \mu - H(\mu),
\end{align*}
</script></div>
where $$H(\mu) = \mathbb{E}_{q(x)} \log q(x)$$ denotes the entropy of $$q(x)$$ and $$\mu = \mathbb{E}_{q(x)} \psi(x)$$ is called the vector of moments. The key observation here is that instead of optimizing over $$q$$, we can equivalently optimize over the set of *valid moments* $$\mu$$.

But which moments are valid? These are the ones that correspond to some probability distribution, i.e. they are in the set
<div class="mathblock"><script type="math/tex; mode=display">
\mathbb{M} = \{ \mu : \exists p \text{ s.t. } \mu = \sum_x \psi(x)p(x) \text{ for some $p(x) \geq 0$ that sums to 1} \}
</script></div>

Notice that $$\mathbb{M}$$ is the intersection of a set of linear constraints: hence it forms a convex set, that we will refer to as the *marginal polytope*. We can thus write the variational inference problem as
<div class="mathblock"><script type="math/tex; mode=display">
\max_\mu \theta^T \mu - H(\mu) \text{ subject to } \mu \in \mathbb{M}.
</script></div>

However, this by itself is not very useful. Although \mathbb{M} is convex, it is hard to describe in general (it is formed by the intersection of an exponential number of hyper-planes). Furthermore, $$H(\mu)$$ may be difficult to compute in general.

We will make this problem feasible by replacing 

-->

<p><br /></p>

<table>
  <tbody>
    <tr>
      <td><a href="../../">Index</a></td>
      <td><a href="../sampling">Previous</a></td>
      <td><a href="../../learning/directed">Next</a></td>
    </tr>
  </tbody>
</table>



    </article>
    <span class="print-footer">Variational inference - Volodymyr Kuleshov</span>
    <footer>
  <hr class="slender">
  <!-- <ul class="footer&#45;links"> -->
  <!--   <li><a href="mailto:hate@spam.net"><span class="icon&#45;mail"></span></a></li>     -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.twitter.com/twitter_handle"><span class="icon-twitter"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//plus.google.com/+googlePlusName"><span class="icon-googleplus"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//github.com/GithubHandle"><span class="icon-github"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.flickr.com/photos/FlickrUserID"><span class="icon-flickr"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="/feed"><span class="icon-feed"></span></a> -->
  <!--     </li> -->
  <!--      -->
  <!-- </ul> -->
<div class="credits">
<!-- <span>&#38;copy; 2018 <!&#45;&#45; &#38;#38;nbsp;&#38;#38;nbsp;VOLODYMYR KULESHOV &#45;&#45;></span></br> <br> -->
<span>Site created with <a href="//jekyllrb.com">Jekyll</a> using the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme</a>. &copy; 2018</span> 
</div>  
</footer>

  </body>
</html>
