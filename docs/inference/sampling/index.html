<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Sampling methods</title>
  <meta name="description" content="Lecture notes for Stanford cs228.">


  <link rel="stylesheet" href="/cs228-notes/css/tufte.css">	
  

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75587219-1', 'auto');
  ga('send', 'pageview');

  </script>

  <link rel="canonical" href="http://localhost:4000/cs228-notes/inference/sampling/">
  <link rel="alternate" type="application/rss+xml" title="Probabilistic graphical modeling course" href="http://localhost:4000/cs228-notes/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
        <a href="/cs228-notes/">Contents</a>
	<a href="http://cs.stanford.edu/~ermon/cs228/index.html">Class</a>
	<a href="http://github.com/ermongroup/cs228-notes">Github</a>
	</nav>
</header>

    <article class="group">
      <h1>Sampling methods</h1>
<p class="subtitle"></p>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      Macros: {
        e: "\\epsilon",
        xti: "x^{(i)}",
        yti: "y^{(i)}",
        bfy: "{\\bf y}",
        bfx: "{\\bf x}",
        bfg: "{\\bf g}",
        bfbeta: "{\\bf \\beta}",
        tp: "\\tilde p",
        pt: "p_\\theta",
        Exp: "{\\mathbb{E}}",
        Ind: "{\\mathbb{I}}",
        KL: "{\\mathbb{KL}}",
        Dc: "{\\mathcal{D}}",
        Tc: "{\\mathcal{T}}",
        Xc: "{\\mathcal{X}}",
        note: ["\\textcolor{blue}{[NOTE: #1]}",1]
      }
    }
  });
</script>


<p>In practice, the probabilistic models that we use are often quite complex, and simple algorithms like variable elimination may be too slow for them. In fact, many interesting classes of models may not admit exact polynomial-time solutions at all, and for this reason, much research effort in machine learning is spent on developing algorithms that yield <em>approximate</em> solutions to the inference problem. This section begins our study of such algorithms.</p>

<p>There exist two main families of approximate algorithms: <em>variational</em> methods<label for="1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="1" class="margin-toggle" /><span class="sidenote">Variational inference methods take their name from the <em>calculus of variations</em>, which deals with optimizing functions that take other functions as arguments. </span>, which formulate inference as an optimization problem, as well as <em>sampling</em> methods, which produce answers by repeatedly generating random numbers from a distribution of interest.</p>

<p>Sampling methods can be used to perform both marginal and MAP inference queries; in addition, they can compute various interesting quantities, such as expectations <script type="math/tex">\Exp[f(X)]</script> of random variables distributed according to a given probabilistic model. Sampling methods have historically been the main way of performing approximate inference, although over the past 15 years variational methods have emerged as viable (and often superior) alternatives.</p>

<h2 id="sampling-from-a-probability-distribution">Sampling from a probability distribution</h2>

<p>As a warm-up, let’s think for a minute how we might sample from a multinomial distribution with <script type="math/tex">k</script> possible outcomes and associated probabilities <script type="math/tex">\theta_1,...,\theta_k</script>.</p>

<p>Sampling, in general, is not an easy problem. Our computers can only generate samples from very simple distributions<label for="1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="1" class="margin-toggle" /><span class="sidenote">Even those samples are not truly random. They are actually taken from a deterministic sequence whose statistical properties (e.g. running averages) are indistinguishable form a truly random one. We call such sequences <em>pseudorandom</em>. </span>, such as the uniform distribution over <script type="math/tex">[0,1]</script>.
All sampling techniques involve calling some kind of simple subroutine multiple times in a clever way.</p>

<p>In our case, we may reduce sampling from a multinomial variable to sampling a single uniform variable by subdividing a unit interval into <script type="math/tex">d</script> regions with region <script type="math/tex">i</script> having size <script type="math/tex">\theta_i</script>. We then sample uniformly from <script type="math/tex">[0,1]</script> and return the value of the region in which our sample falls.</p>
<figure><figcaption>Reducing sampling from a multinomial distribution to sampling a uniform distribution in [0,1].</figcaption><img src="/cs228-notes/assets/img/multinomial-sampling.png" /></figure>

<h3 id="sampling-from-directed-graphical-models">Sampling from directed graphical models</h3>

<p><label for="nb1" class="margin-toggle">⊕</label><input type="checkbox" id="nb1" class="margin-toggle" /><span class="marginnote"><img class="fullwidth" src="/cs228-notes/assets/img/grade-model.png" /><br />Bayes net model describing the performance of a student on an exam. The distribution can be represented a product of conditional probability distributions specified by tables.</span></p>

<p>Our technique for sampling from multinomials naturally extends to Bayesian networks with multinomial variables, via a method called <em>ancestral</em> (or <em>forward</em>) sampling. Given a probability <script type="math/tex">p(x_1,...,x_n)</script> specified by a Bayes net, we sample variables in topological order. In other words, we start by sampling the variables with no parents; then we sample from the next generation by conditioning these variables’ CPDs to values sampled at the first step. We proceed like this until the <script type="math/tex">n</script> variables have been sampled.</p>

<p>In our earlier model of a student’s grade, we would first sample an exam difficulty <script type="math/tex">d'</script> and an intelligence level <script type="math/tex">i'</script>. Then, once we have samples, <script type="math/tex">d', i'</script> we generate a student grade <script type="math/tex">g'</script> from <script type="math/tex">p(g \mid d', i')</script>. At each step, we simply perform standard multinomial sampling.</p>

<h2 id="monte-carlo-estimation">Monte Carlo estimation</h2>

<p>Sampling from a distribution lets us perform many useful tasks, including marginal and MAP inference, as well as computing integrals of the form</p>
<div class="mathblock"><script type="math/tex; mode=display">
\Exp_{x \sim p}[f(x)] = \sum_{x} f(x) p(x).
</script></div>
<p>If <script type="math/tex">g</script> does not have a special structure that matches the Bayes net structure of <script type="math/tex">p</script>, this integral will be impossible to perform analytically; instead, we will approximate it using a large number of samples from <script type="math/tex">p</script>. Algorithms that construct solutions based on a large number of samples from a given distribution are referred to as Monte Carlo (MC) methods<label for="1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="1" class="margin-toggle" /><span class="sidenote">The name Monte Carlo refers to a famous casino in the city of Monaco. The term was originally coined as a codeword by physicists working the atomic bomb as part of the secret Manhattan project. </span>.</p>

<p>Monte Carlo integration is an important instantiation of the general Monte Carlo principle.
This technique approximates a target expectation with</p>
<div class="mathblock"><script type="math/tex; mode=display">
\Exp_{x \sim p}[f(x)] \approx \frac{1}{T} \sum_{t=1}^T f(x^t),
</script></div>
<p>where <script type="math/tex">x^1,...,x^T</script> are samples drawn according to <script type="math/tex">p</script>.</p>

<p>It easy to show that the expected value of <script type="math/tex">I_T</script>, the MC estimate, equals the true integral. We say that <script type="math/tex">I_T</script> is an unbiased estimator for <script type="math/tex">\Exp_{x \sim p}[f(x)]</script>. Moreover as <script type="math/tex">I_T \to \Exp_{x \sim p}[f(x)]</script> as <script type="math/tex">T \to \infty</script>. Finally, we can show that the variance of <script type="math/tex">I_T</script> equals <script type="math/tex">\text{var}_P(f(x))/T</script>, which can be made arbitrarily small with <script type="math/tex">T</script>.</p>

<h3 id="rejection-sampling">Rejection sampling</h3>

<p><label for="nb1" class="margin-toggle">⊕</label><input type="checkbox" id="nb1" class="margin-toggle" /><span class="marginnote"><img class="fullwidth" src="/cs228-notes/assets/img/rejection-sampling.png" /><br />Graphical illustration of rejection sampling. We may compute the area of circle by drawing uniform samples from the square; the fraction of points that fall in the circle represents its area. This method breaks down if the size of the circle is small relative to the size of the square.</span>
A special case of Monte Carlo integration is rejection sampling. We may use it to compute the area of a region <script type="math/tex">R</script> by sampling in a larger region with a known area and recording the fraction of samples that falls within <script type="math/tex">R</script>.</p>

<p>For example, we may use rejection sampling to compute marginal probabilities of the form <script type="math/tex">p(x=x')</script>: we may write this probability as <script type="math/tex">\Exp_{x\sim p}[\Ind(x=x')]</script> and then take the Monte Carlo approximation. This will amount to sampling many samples from <script type="math/tex">p</script> and keeping ones that are consistent with the value of the marginal.</p>

<h3 id="importance-sampling">Importance sampling</h3>

<p>Unfortunately, this procedure is very wasteful. If <script type="math/tex">p(x=x')</script> equals, say, 1%, then we will discard 99% of all samples.</p>

<p>A better way of computing such integrals is via an approach called <em>importance sampling</em>. The main idea is to sample from a distribution <script type="math/tex">q</script> (hopefully roughly proportional to <script type="math/tex">f \cdot p</script>), and then <em>reweigh</em> the samples in a principled way, so that their sum still approximates the desired integral.</p>

<p>More formally, suppose we are interested in computing <script type="math/tex">\Exp_{x \sim p}[f(x)]</script>. We may rewrite this integral as</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align*}
\Exp_{x \sim p}[f(x)] 
& = \sum_{x} f(x) p(x) \\
& = \sum_{x} f(x) \frac{p(x)}{q(x)} q(x) \\
& = \Exp_{x \sim q}[f(x)w(x)] \\
& \approx \frac{1}{T} \sum_{t=1}^T f(x^t) w(x^t) \\
\end{align*}
</script></div>
<p>where <script type="math/tex">w(x) = \frac{p(x)}{q(x)}</script>. In other words, we may instead take samples from <script type="math/tex">q</script> and reweigh them with <script type="math/tex">w(x)</script>; the expected value of this Monte Carlo approximation will be the original integral.</p>

<p>Now the variance of this new estimator equals</p>
<div class="mathblock"><script type="math/tex; mode=display">
\text{var}_{x \sim q}(f(x)w(x)) = \Exp_{x \sim q} [f^2(x) w^2(x)] - \Exp_{x \sim q} [f(x) w(x)]^2 \geq 0
</script></div>
<p>Note that we can set the variance to zero by choosing <span>​<script type="math/tex">q(x) = \frac{|f(x)|p(x)}{\int |f(x)|p(x) dx}</script></span>; this means that if we can sample from this <script type="math/tex">q</script> (and evaluate the corresponding weight), all the Monte Carlo samples will be equal and correspond to the true value of our integral. Of course, sampling from such a <script type="math/tex">q</script> would be NP-hard in general, but this at least gives us an indication for what to strive for.</p>

<p>In the context of our previous example for computing <script type="math/tex">p(x=x') = \Exp_{z\sim p}[p(x'|z)]</script>, we may take <script type="math/tex">q</script> to be the uniform distribution and apply importance sampling as follows:</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align*}
p(x=x')
& = \Exp_{z\sim p}[p(x'|z)] \\
& = \Exp_{z\sim q}[p(x'|z)\frac{p(z)}{q(z)}] \\
& = \Exp_{z\sim q}[\frac{p(x',z)}{q(z)}] \\
& \approx \frac{1}{T} \sum_{t=1}^T \frac{p(z^t, x')}{q(z^t)} \\
\end{align*}
</script></div>
<p>Unlike rejection sampling, this will use all the examples; if <script type="math/tex">p(z|x')</script> is not too far from uniform, this will converge to the true probability after only a very small number of samples.</p>

<h2 id="markov-chain-monte-carlo">Markov chain Monte Carlo</h2>

<p>Let us now turn our attention from computing expectations to performing marginal and MAP inference using sampling.
We will solve these problems using a very powerful technique called Markov chain Monte Carlo<label for="1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="1" class="margin-toggle" /><span class="sidenote">Markov chain Monte Carlo is another algorithm that was developed during the Manhattan project and eventually republished in the scientific literature some decades later. It is so important, that is was recently named as one of the <a href="https://www.siam.org/pdf/news/637.pdf">10 most important algorithms</a> of the XXth century. </span> (MCMC).</p>

<h3 id="markov-chain">Markov Chain</h3>

<p>A key concept in MCMC is that of a <em>Markov chain</em>. A (discrete-time) Markov chain is a sequence of random variables <script type="math/tex">S_0, S_1, S_2, ...</script> with each random variable <script type="math/tex">S_i \in \{1,2,...,d\}</script> taking one of <script type="math/tex">d</script> possible values, intuitively representing the state of a system. The initial state is distributed according to a probability <script type="math/tex">P(S_0)</script>; all subsequent states are generated from a conditional probability distribution that depends only on the previous random state, i.e. <script type="math/tex">S_i</script> is distributed according to <script type="math/tex">P(S_i \mid S_{i-1})</script>.</p>

<p>The probability <script type="math/tex">P(S_i \mid S_{i-1})</script> is the same at every step <script type="math/tex">i</script>; this means that the transition probabilities at any time in the entire process depend only on the given state and not on the history of how we got there. This is called the <em>Markov</em> assumption.</p>

<p><label for="mc" class="margin-toggle">⊕</label><input type="checkbox" id="mc" class="margin-toggle" /><span class="marginnote"><img class="fullwidth" src="/cs228-notes/assets/img/markovchain.png" /><br />A Markov chain over three states. The weighted directed edges indicate probabilities of transitioning to a different state.</span>
It is very convenient to represent the transition probability as a <script type="math/tex">d \times d</script> matrix</p>
<div class="mathblock"><script type="math/tex; mode=display">
T_{ij} = P(S_\text{new} = i \mid S_\text{prev} = j),
</script></div>
<p>where <script type="math/tex">T^t</script> denotes matrix exponentiation (we apply the matrix operator <script type="math/tex">t</script> times).</p>

<p>If the initial state <script type="math/tex">S_0</script> is drawn from a vector probabilities <script type="math/tex">p_0</script>, we may represent the probability <script type="math/tex">p_t</script> of ending up in each state after <script type="math/tex">t</script> steps as</p>
<div class="mathblock"><script type="math/tex; mode=display">
p_t = T^t p_0.
</script></div>

<p>The limit <script type="math/tex">\pi = \lim_{t \to \infty} p_t</script> (when it exists) is called a <em>stationary distribution</em> of the Markov chain. We will construct below Markov chain with a stationary distribution <script type="math/tex">\pi</script> that exists and is the same for all <script type="math/tex">p_0</script>; we will refer to such <script type="math/tex">\pi</script> as <em>the</em> stationary distribution* of the chain.</p>

<p>A sufficent condition for a stationary distribution is called <em>detailed balance</em>:</p>
<div class="mathblock"><script type="math/tex; mode=display">
\pi(x') T(x \mid x') = \pi(x) T(x' \mid x) \;\text{for all $x$}
</script></div>
<p>It is easy to show that such a <script type="math/tex">\pi</script> must form a stationary distribution (just sum both sides of the equation over <script type="math/tex">x</script> and simplify). However, the reverse may not hold and indeed it is possible to have <a href="https://arxiv.org/pdf/1007.2262.pdf">MCMC without satisfying detailed balance</a>.</p>

<h3 id="existence-of-a-stationary-distribution">Existence of a stationary distribution</h3>

<p>The high-level idea of MCMC will be to construct a Markov chain whose states will be joint assignments to the variables in the model and whose stationary distribution will equal the model probability <script type="math/tex">p</script>.</p>

<p>In order to construct such a chain, we first need to understand when stationary distributions exist. This turns out to be true under two sufficient conditions:</p>

<ul>
  <li><em>Irreducibility</em>: It is possible to get from any state <script type="math/tex">x</script> to any other state <script type="math/tex">x'</script> with probability &gt; 0 in a finite number of steps.</li>
  <li><em>Aperiodicity</em>: It is possible to return to any state at any time, i.e. there exists an <script type="math/tex">n</script> such that for all <script type="math/tex">i</script> and all <script type="math/tex">n' \geq n</script>, <script type="math/tex">P(s_{n'}=i \mid s_0 = i) > 0</script>.</li>
</ul>

<p>The first condition is meant to prevent <em>absorbing states</em>, i.e. states from which we can never leave. In the example below, if we start in states <script type="math/tex">1,2</script>, we will never reach state 4. Conversely, if we start in state 4, then we will never reach states 1,2. If we start the chain in the middle (in state 3), then clearly it cannot have a single limiting distribution.</p>
<figure><figcaption>A reducible Markov Chain over four states.</figcaption><img src="/cs228-notes/assets/img/reducible-chain.png" /></figure>

<p>The second condition is necessary to rule out transition operators such as</p>
<div class="mathblock"><script type="math/tex; mode=display">
T = \left[
\begin{matrix}
0 & 1 \\
1 & 0
\end{matrix}
\right].
</script></div>
<p>Note that this chain alternates forever between states 1 and 2 without ever settling in a stationary distribution.</p>

<p><strong>Fact</strong>: An irreducible and aperiodic finite-state Markov chain has a stationary distribution.</p>

<p>In the context of continuous variables, the Markov chain must be <em>ergodic</em>, which is slightly stronger condition than the above (and which requires irreducibility and aperiodicity). For the sake of generality, we will simply require our Markov Chain to be ergodic.</p>

<h3 id="markov-chain-monte-carlo-1">Markov Chain Monte Carlo</h3>

<p>As we said, the idea of MCMC algorithms is to construct a Markov chain over the assignments to a probability function <script type="math/tex">p</script>; the chain will have a stationary distribution equal to <script type="math/tex">p</script> itself; by running the chain for some number of time, we will thus sample from <script type="math/tex">p</script>.</p>

<p>At a high level, MCMC algorithms will have the following structure. They take as argument a transition operator <script type="math/tex">T</script> specifying a Markov chain whose stationary distribution is <script type="math/tex">p</script>, and an initial assignment <script type="math/tex">x_0</script> to the variables of <script type="math/tex">p</script>. An MCMC algorithm then perform the following steps.</p>

<ol>
  <li>Run the Markov chain from <script type="math/tex">x_0</script> for <script type="math/tex">B</script> <em>burn-in</em> steps.</li>
  <li>Run the Markov chain from <script type="math/tex">x_0</script> for <script type="math/tex">N</script> <em>sampling</em> steps and collect all the states that it visits.</li>
</ol>

<p>Assuming <script type="math/tex">B</script> is sufficiently large, the latter collection of states will form samples from <script type="math/tex">p</script>. We may then use these samples for Monte Carlo integration (or in importance sampling). We may also use them to produce Monte Carlo estimates of marginal probabilities. Finally, we may take the sample with the highest probability and use it as an estimate of the mode (i.e. perform MAP inference).</p>

<h3 id="metropolis-hastings-algorithm">Metropolis-Hastings algorithm</h3>

<p>The Metropolis-Hastings (MH) algorithm is our first way to construct Markov chains within MCMC. The MH method constructs a transition operator <script type="math/tex">T(x' \mid x)</script> from two components:</p>

<ul>
  <li>A transition kernel <script type="math/tex">Q(x'\mid x)</script>, specified by the user</li>
  <li>An acceptance probability for moves proposed by <script type="math/tex">Q</script>, specified by the algorithm as</li>
</ul>
<div class="mathblock"><script type="math/tex; mode=display">
A(x' \mid x) = \min \left(1, \frac{P(x')Q(x \mid x')}{P(x)Q(x' \mid x)} \right).
</script></div>

<p>At each step of the Markov chain, we choose a new point <script type="math/tex">x'</script> according to <script type="math/tex">Q</script>. Then, we either accept this proposed change (with probability <script type="math/tex">\alpha</script>), or with probability <script type="math/tex">1-\alpha</script> we remain at our current state.</p>

<p>Notice that the acceptance probability encourages us to move towards more likely points in the distribution (imagine for example that <script type="math/tex">Q</script> is uniform); when <script type="math/tex">Q</script> suggests that we move is a low-probability region, we follow that move only a certain fraction of the time.</p>

<p>In practice, the distribution <script type="math/tex">Q</script> is taken to be something simple, like a Gaussian centered at <script type="math/tex">x</script> if we are dealing with continuous variables.</p>

<p>Given any <script type="math/tex">Q</script> the MH algorithm will ensure that <script type="math/tex">P</script> will be a stationary distribution of the resulting Markov Chain. More precisely, <script type="math/tex">P</script> will satisfy the detailed balance condition with respect to the MH Markov chain.</p>

<p>To see that, first observe that if <script type="math/tex">% <![CDATA[
A(x' \mid x) < 1 %]]></script>, then <script type="math/tex">\frac{P(x)Q(x' \mid x)}{P(x')Q(x \mid x')} > 1</script> and thus <script type="math/tex">A(x \mid x') = 1</script>. When <script type="math/tex">% <![CDATA[
A(x' \mid x) < 1 %]]></script>, this lets us write:</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align*}
A(x' \mid x) & =  \frac{P(x')Q(x \mid x')}{P(x)Q(x' \mid x)} \\
P(x')Q(x \mid x') A(x \mid x') & =  P(x)Q(x' \mid x) A(x' \mid x) \\
P(x')T(x \mid x') & =  P(x)T(x' \mid x),
\end{align*}
</script></div>
<p>which is simply the detailed balance condition. We used <script type="math/tex">T(x \mid x')</script> to denote the full transition operator of MH (obtained by applying both <script type="math/tex">Q</script> and <script type="math/tex">A</script>). Thus, if the MH Markov chain is ergodic, its stationary distribution will be <script type="math/tex">P</script>.</p>

<h3 id="gibbs-sampling">Gibbs sampling</h3>

<p>A widely-used special case of the Metropolis-Hastings methods is Gibbs sampling. Given an ordered set of variables <script type="math/tex">x_1,...,x_n</script> and a starting configuration <script type="math/tex">x^0 = (x_1^0,...,x_n^0)</script>, we iterate through the variables one at a time; at each time step <script type="math/tex">t</script>, we:</p>

<ol>
  <li>Sample <script type="math/tex">x_i' \sim p(x_i \mid x_{-i}^t)</script></li>
  <li>Set <script type="math/tex">x^{t+1} = (x_1^t, ..., x_i', ..., x_n^t).</script></li>
</ol>

<p>We use <script type="math/tex">x_{-i}^t</script> to denote all variables in <script type="math/tex">x^t</script> except <script type="math/tex">x_i</script>. It is often very easy to performing this sampling step, since we only need to condition <script type="math/tex">x_i</script> on its Markov blanket, which is typically small.</p>

<p>Gibbs sampling can be seen as a special case of MH with proposal
<script type="math/tex">Q(x_i', x_{-i} \mid x_i, x_{-i}) = P(x_i' \mid x_{-i}).</script>
It is easy check that the acceptance probability simplifies to one.</p>

<p>Assuming the right transition operator, both Gibbs sampling and MH will eventually produce samples from their stationary distribution, which by construction is <script type="math/tex">P</script>.</p>

<p>There exist simple ways of ensuring that this will be the case</p>

<ul>
  <li>To ensure irreducibility, the transition operator <script type="math/tex">Q</script> with MH should be able to potentially move to every state. In the case of Gibbs sampling, we would like to make sure that every <script type="math/tex">x_i'</script> can get sampled from <script type="math/tex">p(x_i \mid x_{-i}^t)</script>.</li>
  <li>To ensure aperiodicity, it is enough to let the chain transition stay in its state with some probability.</li>
</ul>

<p>In practice, it is not difficult to ensure these requirements are met.</p>

<h3 id="running-time-of-mcmc">Running time of MCMC</h3>

<p>A key parameter to this algorithm in the number of burn-in steps <script type="math/tex">B</script>. Intuitively, this corresponds to the number of steps needed to converge to our limit (stationary) distribution. This is called the <em>mixing time</em> of the Markov chain<label for="1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="1" class="margin-toggle" /><span class="sidenote">There is a technical definition of this quantity, which we will not cover here. </span>. Unfortunately, this time may vary dramatically, and may sometimes take essentially forever. For example, if the transition matrix is</p>
<div class="mathblock"><script type="math/tex; mode=display">
T = \left[
\begin{matrix}
1 -\e & \e \\
\e & 1 -\e
\end{matrix}
\right],
</script></div>
<p>then for small <script type="math/tex">\e</script> it will take a very long time to reach the stationary distribution, which is close to <script type="math/tex">(0.5, 0.5)</script>. At each step, we will stay in the same state with overwhelming probability; very rarely, we will transition to another state, and then stay there for a very long time. The average of these states will converge to <script type="math/tex">(0.5, 0.5)</script>, but the convergence will be very slow.</p>

<p>This problem will also occur with complicated distributions that have two distinct and narrow modes; with high probability, the algorithm will sample from a given mode for a very long time. These examples are indications that sampling is a hard problem in general, and MCMC does not give us a free lunch. Nonetheless, for many real-world distributions, sampling will produce very useful solutions.</p>

<p>Another, perhaps more important problem, is that we may not know when to end the burn-in period, even if it is theoretically not too long. There exist many heuristics to determine whether a Markov chain has <em>mixed</em>; however, typically these heuristics involve plotting certain quantities and estimating them by eye; even the quantitative measures are not significantly more reliable than this approach.</p>

<p>In summary, even though MCMC is able to sample from the right distribution (which in turn can be used to solve any inference problem), doing so may sometimes require a very long time, and there is no easy way to judge the amount of computation that we need to spend to find a good solution.</p>

<p><br /></p>

<table>
  <tbody>
    <tr>
      <td><a href="../../">Index</a></td>
      <td><a href="../map">Previous</a></td>
      <td><a href="../variational">Next</a></td>
    </tr>
  </tbody>
</table>



    </article>
    <span class="print-footer">Sampling methods - Volodymyr Kuleshov</span>
    <footer>
  <hr class="slender">
  <!-- <ul class="footer&#45;links"> -->
  <!--   <li><a href="mailto:hate@spam.net"><span class="icon&#45;mail"></span></a></li>     -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.twitter.com/twitter_handle"><span class="icon-twitter"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//plus.google.com/+googlePlusName"><span class="icon-googleplus"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//github.com/GithubHandle"><span class="icon-github"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.flickr.com/photos/FlickrUserID"><span class="icon-flickr"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="/feed"><span class="icon-feed"></span></a> -->
  <!--     </li> -->
  <!--      -->
  <!-- </ul> -->
<div class="credits">
<!-- <span>&#38;copy; 2017 <!&#45;&#45; &#38;#38;nbsp;&#38;#38;nbsp;VOLODYMYR KULESHOV &#45;&#45;></span></br> <br> -->
<span>Site created with <a href="//jekyllrb.com">Jekyll</a> using the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme</a>. &copy; 2017</span> 
</div>  
</footer>

  </body>
</html>
