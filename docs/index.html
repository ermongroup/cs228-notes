<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Contents</title>
  <meta name="description" content="Lecture notes for Stanford cs228.">


  <link rel="stylesheet" href="/cs228-notes/css/tufte.css">	
  

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75587219-1', 'auto');
  ga('send', 'pageview');

  </script>

  <link rel="canonical" href="http://localhost:4000/cs228-notes/">
  <link rel="alternate" type="application/rss+xml" title="Probabilistic graphical modeling course" href="http://localhost:4000/cs228-notes/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
        <a href="/cs228-notes/">Contents</a>
	<a href="http://cs.stanford.edu/~ermon/cs228/index.html">Class</a>
	<a href="http://github.com/ermongroup/cs228-notes">Github</a>
	</nav>
</header>

    <article class="group">
      <h1>Contents</h1>
<p class="subtitle"></p>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      Macros: {
        e: "\\epsilon",
        xti: "x^{(i)}",
        yti: "y^{(i)}",
        bfy: "{\\bf y}",
        bfx: "{\\bf x}",
        bfg: "{\\bf g}",
        bfbeta: "{\\bf \\beta}",
        tp: "\\tilde p",
        pt: "p_\\theta",
        Exp: "{\\mathbb{E}}",
        Ind: "{\\mathbb{I}}",
        KL: "{\\mathbb{KL}}",
        Dc: "{\\mathcal{D}}",
        Tc: "{\\mathcal{T}}",
        Xc: "{\\mathcal{X}}",
        note: ["\\textcolor{blue}{[NOTE: #1]}",1]
      }
    }
  });
</script>


<p><span class="newthought">These notes</span>  form a concise introductory course on probabilistic graphical models<label for="1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="1" class="margin-toggle" /><span class="sidenote">Probabilistic graphical models are a subfield of machine learning that studies how to describe and reason about the world in terms of probabilities. </span>.
They are based on Stanford <a href="http://cs.stanford.edu/~ermon/cs228/index.html">CS228</a>, taught by <a href="http://cs.stanford.edu/~ermon/">Stefano Ermon</a>, and are written by <a href="http://www.stanford.edu/~kuleshov">Volodymyr Kuleshov</a>, with the <a href="https://github.com/ermongroup/cs228-notes/commits/master">help</a> of many students and course staff.
<label for="mn-id-whatever" class="margin-toggle"> âŠ•</label><input type="checkbox" id="mn-id-whatever" class="margin-toggle" /><span class="marginnote">The notes are still <strong>under construction</strong>!
Although we have written up most of the material, you will probably find several typos. If you do, please let us know, or submit a pull request with your fixes to our <a href="https://github.com/ermongroup/cs228-notes">Github repository</a>. </span>
You too may help make these notes better by submitting your improvements to us via <a href="https://github.com/ermongroup/cs228-notes">Github</a>.</p>

<p>This course starts by introducing probabilistic graphical models from the very basics and concludes by explaining from first principles the <a href="extras/vae">variational auto-encoder</a>, an important probabilistic model that is also one of the most influential recent results in deep learning.</p>

<h2 id="preliminaries">Preliminaries</h2>

<ol>
  <li>
    <p><a href="preliminaries/introduction/">Introduction</a>: What is probabilistic graphical modeling? Overview of the course.</p>
  </li>
  <li>
    <p><a href="preliminaries/probabilityreview">Review of probability theory</a>: Probability distributions. Conditional probability. Random variables (<em>under construction</em>).</p>
  </li>
  <li>
    <p><a href="preliminaries/applications">Examples of real-world applications</a>: Image denoising. RNA structure prediction. Syntactic analysis of sentences. Optical character recognition (<em>under construction</em>).</p>
  </li>
</ol>

<h2 id="representation">Representation</h2>

<ol>
  <li>
    <p><a href="representation/directed/">Bayesian networks</a>: Definitions. Representations via directed graphs. Independencies in directed models.</p>
  </li>
  <li>
    <p><a href="representation/undirected/">Markov random fields</a>: Undirected vs directed models. Independencies in undirected models. Conditional random fields.</p>
  </li>
</ol>

<h2 id="inference">Inference</h2>

<ol>
  <li>
    <p><a href="inference/ve/">Variable elimination</a> The inference problem. Variable elimination. Complexity of inference.</p>
  </li>
  <li>
    <p><a href="inference/jt/">Belief propagation</a>: The junction tree algorithm. Exact inference in arbitrary graphs. Loopy Belief Propagation.</p>
  </li>
  <li>
    <p><a href="inference/map/">MAP inference</a>: Max-sum message passing. Graphcuts. Linear programming relaxations. Dual decomposition.</p>
  </li>
  <li>
    <p><a href="inference/sampling/">Sampling-based inference</a>: Monte-Carlo sampling. Importance sampling. Markov Chain Monte-Carlo. Applications in inference.</p>
  </li>
  <li>
    <p><a href="inference/variational/">Variational inference</a>: Variational lower bounds. Mean Field. Marginal polytope and its relaxations.</p>
  </li>
</ol>

<h2 id="learning">Learning</h2>

<ol>
  <li>
    <p><a href="learning/directed/">Learning in directed models</a>: Maximum likelihood estimation. Learning theory basics. Maximum likelihood estimators for Bayesian networks.</p>
  </li>
  <li>
    <p><a href="learning/undirected/">Learning in undirected models</a>: Exponential families. Maximum likelihood estimation with gradient descent. Learning in CRFs</p>
  </li>
  <li>
    <p><a href="learning/latent/">Learning in latent variable models</a>: Latent variable models. Gaussian mixture models. Expectation maximization.</p>
  </li>
  <li>
    <p><a href="learning/bayesianlearning/">Bayesian learning</a>: Bayesian paradigm. Conjugate priors. Examples (<em>under construction</em>).</p>
  </li>
  <li>
    <p><a href="learning/structLearn/">Structure learning</a>: Chow-Liu algorithm. Akaike information criterion. Bayesian information criterion. Bayesian structure learning (<em>under construction</em>).</p>
  </li>
</ol>

<h2 id="bringing-it-all-together">Bringing it all together</h2>

<ol>
  <li>
    <p><a href="extras/vae">The variational autoencoder</a>: Deep generative models. The reparametrization trick. Learning latent visual representations.</p>
  </li>
  <li>
    <p>List of further readings: Structured support vector machines. Bayesian non-parametrics.</p>
  </li>
</ol>



    </article>
    <span class="print-footer">Contents - Volodymyr Kuleshov</span>
    <footer>
  <hr class="slender">
  <!-- <ul class="footer&#45;links"> -->
  <!--   <li><a href="mailto:hate@spam.net"><span class="icon&#45;mail"></span></a></li>     -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.twitter.com/twitter_handle"><span class="icon-twitter"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//plus.google.com/+googlePlusName"><span class="icon-googleplus"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//github.com/GithubHandle"><span class="icon-github"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.flickr.com/photos/FlickrUserID"><span class="icon-flickr"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="/feed"><span class="icon-feed"></span></a> -->
  <!--     </li> -->
  <!--      -->
  <!-- </ul> -->
<div class="credits">
<!-- <span>&#38;copy; 2017 <!&#45;&#45; &#38;#38;nbsp;&#38;#38;nbsp;VOLODYMYR KULESHOV &#45;&#45;></span></br> <br> -->
<span>Site created with <a href="//jekyllrb.com">Jekyll</a> using the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme</a>. &copy; 2017</span> 
</div>  
</footer>

  </body>
</html>
