<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Learning in latent variable models</title>
  <meta name="description" content="Lecture notes for Stanford cs228.">


  <link rel="stylesheet" href="/cs228-notes/css/tufte.css">	
  

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75587219-1', 'auto');
  ga('send', 'pageview');

  </script>

  <link rel="canonical" href="http://localhost:4000/cs228-notes/learning/latent/">
  <link rel="alternate" type="application/rss+xml" title="Probabilistic graphical modeling course" href="http://localhost:4000/cs228-notes/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
        <a href="/cs228-notes/">Contents</a>
	<a href="http://cs.stanford.edu/~ermon/cs228/index.html">Class</a>
	<a href="http://github.com/ermongroup/cs228-notes">Github</a>
	</nav>
</header>

    <article class="group">
      <h1>Learning in latent variable models</h1>
<p class="subtitle"></p>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      Macros: {
        e: "\\epsilon",
        xti: "x^{(i)}",
        yti: "y^{(i)}",
        bfy: "{\\bf y}",
        bfx: "{\\bf x}",
        bfg: "{\\bf g}",
        bfbeta: "{\\bf \\beta}",
        tp: "\\tilde p",
        pt: "p_\\theta",
        Exp: "{\\mathbb{E}}",
        Ind: "{\\mathbb{I}}",
        KL: "{\\mathbb{KL}}",
        Dc: "{\\mathcal{D}}",
        Tc: "{\\mathcal{T}}",
        Xc: "{\\mathcal{X}}",
        note: ["\\textcolor{blue}{[NOTE: #1]}",1]
      }
    }
  });
</script>


<p>Up to now, we have assumed that when learning a directed or an undirected model, we are given examples of every single variable that we are trying to model.</p>

<p>However, that may not always be the case. Consider for example a probabilistic language model of news articles<label for="1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="1" class="margin-toggle" /><span class="sidenote">A language model <script type="math/tex">p</script> assigns probabilities to sequences of words <script type="math/tex">x_1,...,x_n</script>. We can, among other things, sample from <script type="math/tex">p</script> to generate various kinds of sentences. </span>. Each article <script type="math/tex">x</script> typically focuses on a specific topic <script type="math/tex">t</script>, e.g. finance, sports, politics. 
Using this prior knowledge, we may build a more accurate model <script type="math/tex">p(x|t)p(t)</script>, in which we have introduced an additional, unobserved variable <script type="math/tex">t</script>. This model can be more accurate, because we can now learn a separate <script type="math/tex">p(x|t)</script> for each topic, rather than trying to model everything with one <script type="math/tex">p(x)</script>.</p>

<p>However, since <script type="math/tex">t</script> is unobserved, we cannot directly use the learning methods that we have so far. In fact, the unobserved variables make learning much more difficult; in this chapter, we will look at how to use and how to learn models that involve latent variables.</p>

<h2 id="latent-variable-models">Latent variable models</h2>

<p>More formally, a latent variable model (LVM) <script type="math/tex">p</script> is a probability distribution over two sets of variables <script type="math/tex">x, z</script>:</p>
<div class="mathblock"><script type="math/tex; mode=display">
p(x, z; \theta),
</script></div>
<p>where the <script type="math/tex">x</script> variables are observed at learning time in a dataset <script type="math/tex">D</script> and the <script type="math/tex">z</script> are never observed.</p>

<p>The model may be either directed or undirected. There exist both discriminative and generative LVMs, although here we will focus on the latter (the key ideas hold for discriminative models as well).</p>

<h3 id="example-gaussian-mixture-models">Example: Gaussian mixture models</h3>

<p>Gaussian mixture models (GMMs) are a latent variable model that is also one of the most widely used models in machine learning. <label for="gmm1" class="margin-toggle">⊕</label><input type="checkbox" id="gmm1" class="margin-toggle" /><span class="marginnote"><img class="fullwidth" src="/cs228-notes/assets/img/gmm1.png" /><br />Example of a dataset that is best fit with a mixture of two Gaussians. Mixture models allow us to model clusters in the dataset.</span></p>

<p>In a GMM, each data point is a tuple <script type="math/tex">(x_i, z_i)</script> with <script type="math/tex">x_i \in \mathbb{R}^d</script> and <script type="math/tex">z_i \in {1,2,...,K}</script> (<script type="math/tex">z_i</script> is discrete). The joint <script type="math/tex">p</script> is a directed model</p>
<div class="mathblock"><script type="math/tex; mode=display">
p(x, z) = p(x|z)p(z),
</script></div>
<p>where <script type="math/tex">p(z = k) = \pi_k</script> for some vector of class probabilities <script type="math/tex">\pi \in \Delta_{K-1}</script> and</p>
<div class="mathblock"><script type="math/tex; mode=display">
p(x|z=k) = \mathcal{N}(x; \mu_k, \Sigma_k)
</script></div>
<p>is a multivariate Gaussian with mean and variance <script type="math/tex">\mu_k, \Sigma_k</script>.</p>

<p>This model postulates that our observed data is comprised of <script type="math/tex">K</script> clusters with proportions specified by <script type="math/tex">\pi_1,...,\pi_K</script>; the distribution within each cluster is a Gaussian. We can see that <script type="math/tex">p(x)</script> is a mixture by explicitly writing out this probability:</p>
<div class="mathblock"><script type="math/tex; mode=display">
p(x) = \sum_{k=1}^K p(x|z=k)p(z=k) = \sum_{k=1}^K \pi_k \mathcal{N}(x; \mu_k, \Sigma_k).
</script></div>
<p>To generate a new data point, we sample a cluster <script type="math/tex">k</script> and then sample its Gaussian <script type="math/tex">\mathcal{N}(x; \mu_k, \Sigma_k)</script>.</p>

<figure><figcaption>Example of a Gaussian mixture model, consisting of three components with different class proportions (a). The true class of each point is unobserved, so the distribution over $x$ looks like in (b); it is both multi-modal and non-Gaussian. Visualizing it in 3D shows the effects of class proportions on the magnitudes of the modes.</figcaption><img src="/cs228-notes/assets/img/gmm2.png" /></figure>

<h3 id="why-are-latent-variable-models-useful">Why are latent variable models useful?</h3>

<p>There are two reasons why we might want to use latent variable models.</p>

<p>The simplest reason is that some data might be naturally unobserved. For example, if we are modeling a clinical trial, then some patients may drop out, and we won’t have their measurements. The methods in this chapter can be used to learn with this kind of missing data.</p>

<p>However, the most important reason for studying LVMs is that they enable us to leverage our prior knowledge when defining a model. Our topic modeling example from the introduction illustrates this. We know that our set of news articles is actually a mixture of <script type="math/tex">K</script> distinct distributions (one for each topic); LVMs allow us to design a model that captures this.</p>

<p>LVMs can also be viewed as increasing the expressive power of our model. In the case of GMMs, the distribution that we can model using a mixture of Gaussian components is much more expressive than what we could have modeled using a single component.</p>

<h3 id="marginal-likelihood-training">Marginal likelihood training</h3>

<p>How do we train an LVM? Our goal is still to fit the marginal distribution <script type="math/tex">p(x)</script> over the visible variables <script type="math/tex">x</script> to that observed in our dataset <script type="math/tex">D</script>. Hence our previous discussion about KL divergences applies here as well and by the same argument, we should be maximizing the <em>marginal log-likelihood</em> of the data</p>
<div class="mathblock"><script type="math/tex; mode=display">
\log p(D) = \sum_{x \in D} \log p(x) = \sum_{x \in D} \log \left( \sum_z p(x|z) p(z) \right).
</script></div>

<p>This optimization objective is considerably more difficult than regular log-likelihood, even for directed graphical models. For one, we can see that the summation inside the log makes it impossible to decompose <script type="math/tex">p(x)</script> into a sum of log-factors. Hence, even if the model is directed, we can no longer derive a simple closed form expression for the parameters.</p>

<p><label for="gmm1" class="margin-toggle">⊕</label><input type="checkbox" id="gmm1" class="margin-toggle" /><span class="marginnote"><img class="fullwidth" src="/cs228-notes/assets/img/mixture.png" /><br />Exponential family distributions (gray lines) have concave log-likelihoods. However, a weighted mixture of such distributions is no longer concave (black line).</span>
Looking closer at the distribution of a data point <script type="math/tex">x</script>, we also see that it is actually a mixture</p>
<div class="mathblock"><script type="math/tex; mode=display">
p(x) = \sum_z p(x|z) p(z)
</script></div>
<p>of distributions <script type="math/tex">p(x|z)</script> with weights <script type="math/tex">p(z)</script>. Whereas a single exponential family distribution <script type="math/tex">p(x)</script> has a concave log-likelihood (as we have seen in our discussion of undirected models), the log of a weighted mixture of such distributions is no longer concave or convex.</p>

<p>This non-convexity requires the development of specialized learning algorithms.</p>

<h2 id="learning-latent-variable-models">Learning latent variable models</h2>

<p>Since the objective is non-convex, we will resort to approximate learning algorithms. These methods are widely used in practice and are quite effective<label for="1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="1" class="margin-toggle" /><span class="sidenote">Note however, that (quite surprisingly), many latent variable models (like GMMs) admit algorithms that can compute the global optimum of the maximum likelihood objective, even though it is not convex. Such methods are covered at the end of CS229T. </span>.</p>

<h3 id="the-expectation-maximization-algorithm">The Expectation-Maximization algorithm</h3>

<p>The Expectation-Maximization (EM) algorithm is a hugely important and widely used algorithm for learning directed latent-variable graphical models <script type="math/tex">p(x,z; \theta)</script> with parameters <script type="math/tex">\theta</script> and latent <script type="math/tex">z</script>.</p>

<p>The EM algorithm relies on two simple observations.</p>

<ul>
  <li>If the latent <script type="math/tex">z</script> were fully observed, then we could optimize the log-likelihood exactly using our previously seen closed form solution for <script type="math/tex">p(x,z)</script>.</li>
  <li>Knowing the weights, we can often efficiently compute the posterior <script type="math/tex">p(z\mid x; \theta)</script> (this is an assumption; it is not true for some models).</li>
</ul>

<p>EM follows a simple iterative two-step strategy: given an estimate <script type="math/tex">\theta_t</script> of the weights, compute <script type="math/tex">p(z\mid x)</script> and use it to “hallucinate” values for <script type="math/tex">z</script>. Then, find a new <script type="math/tex">\theta_{t+1}</script> by optimizing the resulting tractable objective. This process will eventually converge.</p>

<p>We haven’t exactly defined what we mean by “hallucinating” the data. The full definition is a bit technical, but its instantiation is very intuitive in most models like GMMs.</p>

<p>By “hallucinating” the data, we mean computing the expected log-likelihood</p>
<div class="mathblock"><script type="math/tex; mode=display">
\mathbb{E}_{z \sim p(z|x)} \log p(x,z; \theta).
</script></div>
<p>This expectation is what gives the EM algorithm half of its name. If <script type="math/tex">z</script> is not too high-dimensional (e.g. in GMMs it is a one-dimensional categorical variable), then we can compute this expectation.</p>

<p>Since the summation is now outside the log, we can maximize the expected log-likelihood. In particular, when <script type="math/tex">p</script> is a directed model, <script type="math/tex">\log p</script> again decomposes into a sum of log-CPD terms that can be optimized independently, as discussed in the chapter on directed graphical models.</p>

<p>We can formally define the EM algorithm as follows. Let <script type="math/tex">D</script> be our dataset.</p>

<ul>
  <li>Starting at an initial <script type="math/tex">\theta_0</script>, repeat until convergence for <script type="math/tex">t=1,2,...</script>:</li>
  <li><em>E-Step</em>: For each <script type="math/tex">x \in D</script>, compute the posterior <script type="math/tex">p(z\mid x; \theta_{t})</script>.</li>
  <li><em>M-Step</em>: Compute new weights via</li>
</ul>
<div class="mathblock"><script type="math/tex; mode=display">
\theta_{t+1} = \arg \max_{\theta} \sum_{x \in D} \mathbb{E}_{z \sim p(z|x; \theta_{t})} \log p(x,z; \theta).
</script></div>

<h3 id="example-gaussian-mixture-models-1">Example: Gaussian mixture models</h3>

<p>Let’s look at this algorithm in the context of GMMs. Suppose we have a dataset <script type="math/tex">D</script>. In the E-step, we may compute the posterior for each data point <script type="math/tex">x</script> as follows:</p>
<div class="mathblock"><script type="math/tex; mode=display">
p(z \mid x; \theta_{t}) = \frac{p(z, x; \theta_{t})}{p(x; \theta_{t})} = \frac{p(x | z; \theta_{t}) p(z; \theta_{t})}{\sum_{k=1}^K p(x | z_k; \theta_{t}) p(z_k; \theta_{t})}.
</script></div>

<p>Note that each <script type="math/tex">p(x \mid z_k; \theta_{t}) p(z_k; \theta_{t})</script> is simply the probability that <script type="math/tex">x</script> originates from component <script type="math/tex">k</script> given the current set of parameters <script type="math/tex">\theta</script>. After normalization, these form the <script type="math/tex">K</script>-dimensional vector of probabilities <script type="math/tex">p(z \mid x; \theta_{t})</script>.</p>

<p>Recall that in the original model, <script type="math/tex">z</script> is an indicator variable that chooses a component for <script type="math/tex">x</script>; we may view this as a “hard” assignment of <script type="math/tex">x</script> to one component. The result of the <script type="math/tex">E</script> step is a <script type="math/tex">K</script>-dimensional vector (whose components sum to one) that specifies a “soft” assignment to components. In that sense, we have “hallucinated” a “soft” instantiation of <script type="math/tex">z</script>; this is what we meant earlier by an “intuitive interpretation” for <script type="math/tex">p(z\mid x; \theta_{t})</script>.</p>

<p>At the M-step, we optimize the expected log-likelihood of our model.</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{align*}
\theta_{t+1} 
& = \arg \max_{\theta} \sum_{x \in D} \mathbb{E}_{z \sim p(z|x; \theta_{t})} \log p(x,z; \theta) \\
& = \arg \max_{\theta} \sum_{k=1}^K \sum_{x \in D} p(z_k|x; \theta_{t}) \log p(x|z_k; \theta) + \sum_{k=1}^K \sum_{x \in D} p(z_k|x; \theta_{t}) \log p(z_k; \theta)
\end{align*}
</script></div>

<p>We can optimize each of these terms separately. We will start with <script type="math/tex">p(x\mid z_k; \theta) = \mathcal{N}(x; \mu_k, \Sigma_k)</script>. We have to find <script type="math/tex">\mu_k, \Sigma_k</script> that maximize</p>
<div class="mathblock"><script type="math/tex; mode=display">
\sum_{x \in D} p(z_k|x; \theta_{t}) \log p(x|z_k; \theta)
= c_k \cdot \mathbb{E}_{x \sim Q_k(x)} \log p(x|z_k; \theta),
</script></div>
<p>where <script type="math/tex">c_k = \sum_{x \in D} p(z_k|x; \theta_t)</script> is a constant that does not depend on <script type="math/tex">\theta</script> and <script type="math/tex">Q_k(x)</script> is a probability distribution defined over <script type="math/tex">D</script> as</p>
<div class="mathblock"><script type="math/tex; mode=display">
Q_k(x) = \frac{p(z_k|x; \theta_t)}{\sum_{x \in D} p(z_k|x; \theta_t)}.
</script></div>
<p>Now we know that <script type="math/tex">\mathbb{E}_{x \sim Q_k(x)} \log p(x|z_k; \theta)</script> is optimized when <script type="math/tex">p(x|z_k; \theta)</script> equals <script type="math/tex">Q_k(x)</script> (as discussed in the section on learning directed models, this objective equals the KL divergence between <script type="math/tex">Q_k</script> and <script type="math/tex">P</script>, plus a constant).
Moreover, since <script type="math/tex">p(x\mid z_k; \theta) = \mathcal{N}(x; \mu_k, \Sigma_k)</script> is in the exponential family, it is entirely described by its sufficient statistics (recall our discussion of exponential families in the section on learning undirected models). Thus, we may set the mean and variance <script type="math/tex">\mu_k, \Sigma_k</script> to those of <script type="math/tex">Q_k(x)</script>, which are</p>
<div class="mathblock"><script type="math/tex; mode=display">
\mu_k = \mu_{Q_k} = \sum_{x \in D} \frac{ p(z_k|x; \theta_t)}{\sum_{x \in D} p(z_k|x; \theta_t)} x
</script></div>
<p>and</p>
<div class="mathblock"><script type="math/tex; mode=display">
\Sigma_k = \Sigma_{Q_k} = \sum_{x \in D} \frac{ p(z_k|x; \theta_t)}{\sum_{x \in D} p(z_k|x; \theta_t)} (x-\mu_{Q_k}) (x-\mu_{Q_k})^T.
</script></div>

<p>Note how these are the just the mean and variance of the data, weighted by their cluster affinities! Similarly, we may find out that the class priors are</p>
<div class="mathblock"><script type="math/tex; mode=display">
\pi_k = \frac{1}{|D|}\sum_{x \in D} p(z_k|x; \theta_t).
</script></div>

<p>Although we have derived these results using general facts about exponential families, it’s equally possible to derive them using standard calculus techniques.</p>

<h3 id="em-as-variational-inference">EM as variational inference</h3>

<p>Why exactly does EM converge? We can understand the behavior of EM by casting it in the framework of variational inference.</p>

<p>Consider the posterior inference problem for <script type="math/tex">p(z\mid x)</script>, where the <script type="math/tex">x</script> variables are held fixed as evidence. We may apply our variational inference framework by taking <script type="math/tex">p(x,z)</script> to be the unnormalized distribution; in that case, <script type="math/tex">p(x)</script> will be the normalization constant.</p>

<p>Recall that variational inference maximizes the evidence lower bound (ELBO)</p>
<div class="mathblock"><script type="math/tex; mode=display">
\mathcal{L}(p,q) = \mathbb{E}_{q(z)} \left[ \log p(x,z; \theta) - \log q(z) \right]
</script></div>
<p>over distributions <script type="math/tex">q</script>. The ELBO satisfies the equation</p>
<div class="mathblock"><script type="math/tex; mode=display">
\log p(x; \theta) = KL(q(z) || p(z|x; \theta)) + \mathcal{L}(p,q).
</script></div>
<p>Hence, <script type="math/tex">\mathcal{L}(p,q)</script> is maximized when <script type="math/tex">q=p(z|x)</script>; in that case the KL term becomes zero and the lower bound is tight: <script type="math/tex">\log p(x; \theta) = \mathcal{L}(p,q).</script></p>

<p>The EM algorithm can be seen as iteratively optimizing the ELBO over <script type="math/tex">q</script> (at the E step) and over <script type="math/tex">\theta</script> (at the M) step.</p>

<p>Starting at some <script type="math/tex">\theta_t</script>, we compute the posterior <script type="math/tex">p(z\mid x; \theta)</script> at the <script type="math/tex">E</script> step. We evaluate the ELBO for <script type="math/tex">q = p(z\mid x; \theta)</script>; this makes the ELBO tight:</p>
<div class="mathblock"><script type="math/tex; mode=display">
\log p(x; \theta_t) = \mathbb{E}_{p(z|x; \theta_t)} \log p(x,z; \theta_t) - \mathbb{E}_{p(z|x; \theta_t)} \log p(z|x; \theta_t)
</script></div>
<p>Next, we optimize the ELBO over <script type="math/tex">p</script>, holding <script type="math/tex">q</script> fixed. We solve the problem</p>
<div class="mathblock"><script type="math/tex; mode=display">
\theta_{t+1} = \arg\max_\theta \mathbb{E}_{p(z|x; \theta_t)} \log p(x,z; \theta) - \mathbb{E}_{p(z|x; \theta_t)} \log p(z|x; \theta_t)
</script></div>
<p>Note that this is precisely the optimization problem solved at the <script type="math/tex">M</script> step of EM (in the above equation, there is an additive constant independent of <script type="math/tex">\theta</script>).</p>

<p>Solving this problem increases the ELBO. However, since we fixed <script type="math/tex">q</script> to <script type="math/tex">\log p(z\mid x; \theta_t)</script>, the ELBO evaluated at the new <script type="math/tex">\theta_{t+1}</script> is no longer tight. But since the ELBO was equal to <script type="math/tex">\log p(x ;\theta_t)</script> before optimization, we know that the true log-likelihood <script type="math/tex">\log p(x;\theta_{t+1})</script> must have increased.</p>

<p>We now repeat this procedure, computing <script type="math/tex">p(z\mid x; \theta_{t+1})</script> (the E-step), plugging <script type="math/tex">p(z\mid x; \theta_{t+1})</script> into the ELBO (which makes the ELBO tight), and maximizing the resulting expression over <script type="math/tex">\theta</script>. Every step increases the marginal likelihood <script type="math/tex">\log p(x;\theta_t)</script>, which is what we wanted to show.</p>

<h3 id="properties-of-em">Properties of EM</h3>

<p>From our above discussion, it follows that EM has the following properties:</p>

<ul>
  <li>The marginal likelihood increases after each EM cycle.</li>
  <li>Since the marginal likelihood is upper-bounded by its true global maximum, and it increases at every step, EM must eventually converge.</li>
</ul>

<p>However, since we optimizing a non-convex objective, we have no guarantee to find the global optimum. In fact, EM in practice converges almost always to a local optimum, and moreover, that optimum heavily depends on the choice of initialization. Different initial <script type="math/tex">\theta_0</script> can lead to very different solutions, and so it is very common to use multiple restarts of the algorithm and choose the best one in the end. In fact EM is so sensitive to the choice of initial parameters, that techniques for choosing these parameters are still an active area of research.</p>

<p>In summary, the EM algorithm is a very popular technique for optimizing latent variable models that is also often very effective. Its main downside are its difficulties with local minima.</p>

<p><br /></p>

<table>
  <tbody>
    <tr>
      <td><a href="../../">Index</a></td>
      <td><a href="../undirected">Previous</a></td>
      <td><a href="../bayesianlearning">Next</a></td>
    </tr>
  </tbody>
</table>



    </article>
    <span class="print-footer">Learning in latent variable models - Volodymyr Kuleshov</span>
    <footer>
  <hr class="slender">
  <!-- <ul class="footer&#45;links"> -->
  <!--   <li><a href="mailto:hate@spam.net"><span class="icon&#45;mail"></span></a></li>     -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.twitter.com/twitter_handle"><span class="icon-twitter"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//plus.google.com/+googlePlusName"><span class="icon-googleplus"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//github.com/GithubHandle"><span class="icon-github"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.flickr.com/photos/FlickrUserID"><span class="icon-flickr"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="/feed"><span class="icon-feed"></span></a> -->
  <!--     </li> -->
  <!--      -->
  <!-- </ul> -->
<div class="credits">
<!-- <span>&#38;copy; 2018 <!&#45;&#45; &#38;#38;nbsp;&#38;#38;nbsp;VOLODYMYR KULESHOV &#45;&#45;></span></br> <br> -->
<span>Site created with <a href="//jekyllrb.com">Jekyll</a> using the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme</a>. &copy; 2018</span> 
</div>  
</footer>

  </body>
</html>
