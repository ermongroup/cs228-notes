<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Bayesian Learning</title>
  <meta name="description" content="Lecture notes for Stanford cs228.">


  <link rel="stylesheet" href="/cs228-notes/css/tufte.css">	
  

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75587219-1', 'auto');
  ga('send', 'pageview');

  </script>

  <link rel="canonical" href="http://localhost:4000/cs228-notes/learning/bayesianlearning/">
  <link rel="alternate" type="application/rss+xml" title="Probabilistic graphical modeling course" href="http://localhost:4000/cs228-notes/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
        <a href="/cs228-notes/">Contents</a>
	<a href="http://cs.stanford.edu/~ermon/cs228/index.html">Class</a>
	<a href="http://github.com/ermongroup/cs228-notes">Github</a>
	</nav>
</header>

    <article class="group">
      <h1>Bayesian learning</h1>
<p class="subtitle"></p>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      Macros: {
        e: "\\epsilon",
        xti: "x^{(i)}",
        yti: "y^{(i)}",
        bfy: "{\\bf y}",
        bfx: "{\\bf x}",
        bfg: "{\\bf g}",
        bfbeta: "{\\bf \\beta}",
        tp: "\\tilde p",
        pt: "p_\\theta",
        Exp: "{\\mathbb{E}}",
        Ind: "{\\mathbb{I}}",
        KL: "{\\mathbb{KL}}",
        Dc: "{\\mathcal{D}}",
        Tc: "{\\mathcal{T}}",
        Xc: "{\\mathcal{X}}",
        note: ["\\textcolor{blue}{[NOTE: #1]}",1]
      }
    }
  });
</script>


<p>The learning approaches we have discussed so far are based on the principle of maximum likelihood estimation. While being extremely general, there are limitations of this approach as illustrated in the two examples below.</p>

<h2 id="example-1">Example 1</h2>

<p>Let’s suppose we are interested in modeling the outcome of a biased coin, <script type="math/tex">X = \{heads, tails\}</script>. We toss the coin 10 times, observing 6 heads. If <script type="math/tex">\theta</script> denotes the probability of observing heads, the maximum likelihood estimate (MLE) is given by,</p>

<div class="mathblock"><script type="math/tex; mode=display">
\theta_{MLE} = \frac{num\_heads}{num\_heads + num\_tails} = 0.6
</script></div>

<p>Now, suppose we continue tossing the coin such that after a 100 total trials (including the 10 initial trials), we observe 60 heads. Again, we can compute the MLE as,</p>

<div class="mathblock"><script type="math/tex; mode=display">
\theta_{MLE} = \frac{num\_heads}{num\_heads + num\_tails} = 0.6
</script></div>

<p>In both the above situations, the maximum likelihood estimate does not change as we observe more data. This seems counterintuitive - our <em>confidence</em> in predicting heads with probability 0.6 should be higher in the second setting where we have seen many more trials of the coin! The reason why MLE fails to distinguish the two settings is due to an implicit assumption we have been making all along. MLE assumes that the only source of uncertainty is due to the variables, <script type="math/tex">X</script> and the quantification of this uncertainty is based on a fixed parameter <script type="math/tex">\theta_{MLE}</script>.</p>

<h2 id="example-2">Example 2</h2>

<p>Consider a language model for sentences based on the bag-of-words assumption. In such a model, the probability of a sentence can be factored as the probability of the words appearing in the sentence.</p>

<p>For simplicity, assume that our language corpus consists of a single sentence, “Probabilistic graphical models are fun. They are also powerful.”. We can estimate the probability of each of the individual words based on the counts. Our corpus contains 10 words with each word appearing once, and hence, each word in the corpus is assigned a probability of 0.1. Now, while testing the generalization of our model to the English language, we observe another sentence “Probabilistic graphical models are hard.”. The probability of the sentence under our model is 
<script type="math/tex">0.1 \times 0.1 \times 0.1 \times 0.1 \times 0 = 0</script>. We did not observe one of the words (“hard”) during training which made our language model infer the sentence as impossible, even though it is a perfectly plausible sentence.</p>

<p>Out-of-vocabulary words are a common phenomena even for language models trained on large corpus. One of the simplest ways to handle these words is to assign a prior probability of observing an out-of-vocabulary word such that the model will assign a low, but non-zero probability to test sentences containing such words. This mechanism of incorporating prior knowledge is a practical application of Bayesian learning, which we present next.</p>

<h2 id="setup">Setup</h2>

<p>In contrast to maximum likelihood learning, Bayesian learning explicitly models uncertainty over both the variables, <script type="math/tex">X</script> and the parameters, <script type="math/tex">\theta</script>.  In other words, the model parameters <script type="math/tex">\theta</script> are random variables as well.</p>

<p>A <em>prior</em> distribution over the parameters, <script type="math/tex">p(\theta)</script> encodes our initial beliefs. These beliefs are subjective. For example, we can choose the prior over <script type="math/tex">\theta</script> for a biased coin to be uniform between 0 and 1. If however we expect the coin to be fair, the prior distribution can be peaked around <script type="math/tex">\theta = 0.5</script>. We will discuss commonly used priors later in this chapter.</p>

<p>Observing data <script type="math/tex">D</script> in the form of evidence allows us to update our beliefs using Bayes’ rule,</p>

<div class="mathblock"><script type="math/tex; mode=display">
p(\theta \mid D) = \frac{p(D \mid \theta) \, p(\theta)}{p(D)} \propto p(D \mid \theta) \, p(\theta)
</script></div>

<div class="mathblock"><script type="math/tex; mode=display">
posterior \propto likelihood \times prior
</script></div>

<p>Hence, Bayesian learning provides a principled mechanism for incorporating prior knowledge into our model. This prior knowledge is useful in many situations such as when want to provide uncertainty estimates about the model parameters (Example 1) or when the data available for learning a model is limited (Example 2).</p>

<h2 id="conjugate-priors">Conjugate Priors</h2>
<p>When calculating posterior distribution using Bayes’ rule, as in the above, it should be pretty straightforward to calculate the numerator. But to calculate the denominator <script type="math/tex">P(D)</script>, we are required to compute an integral. This might cause us trouble, since for an arbitrary distribution, computing the integral is likely to be intractable.</p>

<p>To tackle this issue, we use a conjugate prior. A parametric family <script type="math/tex">\varphi</script> is conjugate for the likelihood <script type="math/tex">P(D \mid \theta)</script> if:</p>

<div class="mathblock"><script type="math/tex; mode=display">
P(\theta) \in \varphi \Longrightarrow P(\theta \mid D) \in \varphi
</script></div>

<p>This is convenient because if we know the normalizing constant of <script type="math/tex">\varphi</script>, then we get the denominator in Bayes’ rule “for free”. Thus it essentially reduces the computation of the posterior from a tricky numerical integral to some simple algebra.</p>

<p>To see conjugate prior in action, let’s consider an example. Suppose we are given a sequence of <script type="math/tex">N</script> coin tosses, <script type="math/tex">D = \{X_{1},...,X_{N}\}</script>. We want to infer the probability of getting heads which we denote by <script type="math/tex">\theta</script>.  Now, we can model this as a sequence of Bernoulli trials with parameter <script type="math/tex">\theta</script>. A natural conjugate prior in this case is the beta distribution with</p>

<div class="mathblock"><script type="math/tex; mode=display">
P(\theta) = Beta(\theta \mid \alpha_{H}, \alpha_{T}) = \frac{\theta^{\alpha_{H} -1 }(1-\theta)^{\alpha_{T} -1 }}{B(\alpha_{H},\alpha_{T})}
</script></div>

<p>where the normalization constant <script type="math/tex">B(\cdot)</script> is the beta function. Here <script type="math/tex">\alpha = (\alpha_{H},\alpha_{T})</script> are called the hyperparameters of the prior. The expected value of <script type="math/tex">\theta</script> is <script type="math/tex">\frac{\alpha_{H}}{\alpha_{H}+\alpha_{T}}</script>. Here the sum of the hyperparameters <script type="math/tex">(\alpha_{H}+\alpha_{T})</script> can be interpreted as a measure of confidence in the expectations they lead to. Intuitively, we can think of <script type="math/tex">\alpha_{H}</script> as the number of heads we have observed before the current dataset.</p>

<p><label for="nb1" class="margin-toggle">⊕</label><input type="checkbox" id="nb1" class="margin-toggle" /><span class="marginnote"><img class="fullwidth" src="/cs228-notes/assets/img/beta.png" /><br />Here the exponents <script type="math/tex">(3,2)</script> and <script type="math/tex">(30,20)</script> can both be used to encode the belief that <script type="math/tex">\theta</script> is <script type="math/tex">0.6.</script> But the second set of exponents imply a stronger belief as they are based on a larger sample.</span></p>

<p>Out of <script type="math/tex">N</script> coin tosses, if the number of heads and the number of tails are <script type="math/tex">N_{H}</script>
and <script type="math/tex">N_{T}</script> respectively, then it can be shown that the posterior is:</p>

<div class="mathblock"><script type="math/tex; mode=display">
P(\theta \mid N_{H}, N_{T}) = \frac{\theta^{N_{H}+ \alpha_{H} -1 }(1-\theta)^{ N_{T}+ \alpha_{T} -1 }}{B(N_{H}+ \alpha_{H},N_{T}+ \alpha_{T})}
</script></div>

<p>which is another Beta distribution with parameters <script type="math/tex">(N_{H}+ \alpha_{H},N_{T}+ \alpha_{T})</script>. We can use this posterior distribution as the prior for more samples with the hyperparameters simply adding each extra piece of information as it comes from additional coin tosses.</p>

<h3 id="categorical-generalization">Categorical Generalization</h3>

<p>We can now extend the binary model to its categorical generalization.  Instead of being limited to binary outcomes, we can now consider the categorical dataset of a <script type="math/tex">K</script>-sided dice rolled <script type="math/tex">N</script> times.  Let <script type="math/tex">\mathcal{D} = \{ X_1 = k_1, ..., X_N = K_N \}</script>, where <script type="math/tex">X_j \in \{ 1, ..., K \}</script> for the <script type="math/tex">j</script>th outcome.  The parameterization of the model is <script type="math/tex">\theta = (P(X_j = 1), ..., P(X_j = K))</script>, which denotes the probability of each outcome, and where <script type="math/tex">\sum_{k = 1}^K P(X_j = k) = 1</script>.</p>

<p>The likelihood of observing our dataset given a specific parameterization is</p>

<div class="mathblock"><script type="math/tex; mode=display">
P(\mathcal{D} \mid \theta) = \prod_{k=1}^K P(X_j = k)^{\sum_{j=1}^N  1\{ X_j = k \}}
</script></div>

<p>In the same manner as with the binary model, the conjugate prior for this categorical model is the Dirichlet distribution, which has hyperparameters <script type="math/tex">\mathbf{\alpha} = (\alpha_1, ..., \alpha_K)</script>, indicating the number of observations of each outcome.  Letting <script type="math/tex">\alpha</script> be the “virtual” counts of the <script type="math/tex">K</script> outcomes before we observe the dataset, our prior is</p>

<div class="mathblock"><script type="math/tex; mode=display">
P(\theta) = \textsf{Dirichlet}(\theta \mid \mathbf{\alpha}) = \frac{1}{B(\alpha)} \prod_{k=1}^K P(X_j = k)^{\alpha_k - 1}
</script></div>

<p>where <script type="math/tex">B(\cdot)</script> is still a normalization factor.</p>

<p>Because we use a Dirichlet prior, the posterior is also a Dirichlet distribution, and is formulated as follows:</p>

<div class="mathblock"><script type="math/tex; mode=display">
P(\theta \mid \mathcal{D}) \propto P(\mathcal{D} \mid \theta) P(\theta) \propto \prod_{k=1}^K P(X_j = k)^{\sum_{j=1}^N  1\{ X_j = k \} + \alpha_k - 1}
</script></div>

<p>We can see that this is equivalent to a Dirichlet distribution with updated counts <script type="math/tex">\alpha'</script>.  Specifically,</p>

<div class="mathblock"><script type="math/tex; mode=display">
P(\theta \mid \mathcal{D}) \propto \mathsf{Dirichlet}(\theta \mid \alpha')
</script></div>

<p>where the updated count <script type="math/tex">\alpha'</script> is given by</p>

<div class="mathblock"><script type="math/tex; mode=display">
\alpha'_k = \underbrace{\sum_{j=1}^N 1\{ X_j = k \}}_\text{observed data count} + \underbrace{\alpha_k}_\text{prior virtual count}
</script></div>

<p><br /></p>

<table>
  <tbody>
    <tr>
      <td><a href="../../">Index</a></td>
      <td><a href="../latent">Previous</a></td>
      <td><a href="../structLearn">Next</a></td>
    </tr>
  </tbody>
</table>



    </article>
    <span class="print-footer">Bayesian Learning - Volodymyr Kuleshov</span>
    <footer>
  <hr class="slender">
  <!-- <ul class="footer&#45;links"> -->
  <!--   <li><a href="mailto:hate@spam.net"><span class="icon&#45;mail"></span></a></li>     -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.twitter.com/twitter_handle"><span class="icon-twitter"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//plus.google.com/+googlePlusName"><span class="icon-googleplus"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//github.com/GithubHandle"><span class="icon-github"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.flickr.com/photos/FlickrUserID"><span class="icon-flickr"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="/feed"><span class="icon-feed"></span></a> -->
  <!--     </li> -->
  <!--      -->
  <!-- </ul> -->
<div class="credits">
<!-- <span>&#38;copy; 2018 <!&#45;&#45; &#38;#38;nbsp;&#38;#38;nbsp;VOLODYMYR KULESHOV &#45;&#45;></span></br> <br> -->
<span>Site created with <a href="//jekyllrb.com">Jekyll</a> using the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme</a>. &copy; 2018</span> 
</div>  
</footer>

  </body>
</html>
