<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Real-World Applications</title>
  <meta name="description" content="Lecture notes for Stanford cs228.">


  <link rel="stylesheet" href="/cs228-notes/css/tufte.css">	
  

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75587219-1', 'auto');
  ga('send', 'pageview');

  </script>

  <link rel="canonical" href="http://localhost:4000/cs228-notes/preliminaries/applications/">
  <link rel="alternate" type="application/rss+xml" title="Probabilistic graphical modeling course" href="http://localhost:4000/cs228-notes/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
        <a href="/cs228-notes/">Contents</a>
	<a href="http://cs.stanford.edu/~ermon/cs228/index.html">Class</a>
	<a href="http://github.com/ermongroup/cs228-notes">Github</a>
	</nav>
</header>

    <article class="group">
      <h1>Real-world applications</h1>
<p class="subtitle"></p>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      Macros: {
        e: "\\epsilon",
        xti: "x^{(i)}",
        yti: "y^{(i)}",
        bfy: "{\\bf y}",
        bfx: "{\\bf x}",
        bfg: "{\\bf g}",
        bfbeta: "{\\bf \\beta}",
        tp: "\\tilde p",
        pt: "p_\\theta",
        Exp: "{\\mathbb{E}}",
        Ind: "{\\mathbb{I}}",
        KL: "{\\mathbb{KL}}",
        Dc: "{\\mathcal{D}}",
        Tc: "{\\mathcal{T}}",
        Xc: "{\\mathcal{X}}",
        note: ["\\textcolor{blue}{[NOTE: #1]}",1]
      }
    }
  });
</script>


<p>Probabilistic graphical models have numerous and diverse real-world applications.  We provide an overview of the following applications of probabilistic graphical models, which are just a few examples of their many practical uses.</p>

<ul>
  <li><strong>Images</strong>
    <ul>
      <li><a href="#image-generation">Generation</a></li>
      <li><a href="#image-inpainting">In-Painting</a></li>
      <li><a href="#image-denoising">Denoising</a></li>
    </ul>
  </li>
  <li><strong>Language</strong>
    <ul>
      <li><a href="#text-generation">Generation</a></li>
      <li><a href="#text-translation">Translation</a></li>
    </ul>
  </li>
  <li><strong>Audio</strong>
    <ul>
      <li><a href="#audio-superresolution">Super-Resolution</a></li>
      <li><a href="#speech-synthesis">Speech Synthesis</a></li>
      <li><a href="#speech-recognition">Speech Recognition</a></li>
    </ul>
  </li>
  <li><strong>Science</strong>
    <ul>
      <li><a href="#error-correcting-codes">Error-Correcting Codes</a></li>
      <li><a href="#comp-bio">Computational Biology</a></li>
      <li><a href="#ecology">Ecology</a></li>
      <li><a href="#economics">Economics</a></li>
    </ul>
  </li>
</ul>

<h2 id="probabilistic-models-of-images">Probabilistic Models of Images</h2>

<p>Consider a distribution <script type="math/tex">p(\mathbf{x})</script> over images, where <script type="math/tex">\mathbf{x}</script> is an image represented as a vector of pixels, that assigns high probability to images that look realistic and low probability to everything else. Given such a model, we can solve a wide array of interesting tasks.</p>

<p><a id="image-generation"></a></p>
<h3 id="image-generation">Image Generation</h3>

<p><a href="https://arxiv.org/abs/1710.10196">Radford et al.</a> trained a probabilistic model <script type="math/tex">p(\mathbf{x})</script> that assigns high probability to images that look like bedrooms.  To do so, they trained their model on a dataset of bedroom images, a sample of which is shown below:</p>

<p><strong>Training Data</strong><br /> 
<img src="bedroominpainting1.png" alt="bedroom1" /><br /></p>

<p>Now that we have this probabilistic model of bedrooms, we can now <em><strong>generate</strong></em> new realistic bedroom images by sampling from this distribution.  Specifically, new sampled images <script type="math/tex">\hat{\mathbf{x}} \sim p(\mathbf{x})</script> are created directly from our model <script type="math/tex">p(\mathbf{x})</script>, which can now generate data similar to the bedroom images that we trained it with.</p>

<p>Moreover, one of the reasons why generative models are powerful lies in the fact that they have many fewer parameters than the amount of data that they are trained with — as a result, the models have to efficiently distill the essence of the training data to be able to generate new samples.  We see that our particular probabilistic model of bedrooms has done a good job of capturing the data’s essence, and can therefore produce highly realistic images, some examples of which are shown below:</p>

<p><strong>Generated Data</strong><br /> 
<img src="bedroominpainting2.png" alt="bedroom2" /></p>

<p>Similiarly, we can learn a model for faces.</p>

<p><img src="progressiveGAN.png" alt="faces1" /></p>

<p>As with the bedroom images, these faces are completely synthetic — these images are not of an actual person.</p>

<p>The same approach can be used for other objects.</p>

<p><img src="pnpgan.png" alt="faces1" /></p>

<p>Note that the images are not perfect and may need to be refined; however, sampling generates images that are very similiar to what one might expect.</p>

<p><a id="image-inpainting"></a></p>
<h3 id="in-painting">In-Painting</h3>

<p>Using the same <script type="math/tex">p(\mathbf{x})</script> for faces as before, we can also “fill in” the rest of an image. For example, given <script type="math/tex">p(\mathbf{x})</script> and a patch of an existing image (e.g., a piece of a photograph), we can sample from <script type="math/tex">p(\textsf{image} \mid \textsf{patch})</script> and generate different possible ways of completing the image:</p>

<p><img src="inpainting3.png" alt="inpainting2" /></p>

<p>Note the importance of a probabilistic model that captures uncertainty: there could be multiple ways to complete the image!</p>

<p><a id="image-denoising"></a></p>
<h3 id="image-denoising">Image Denoising</h3>

<p>Similarly, given an image corrupted by noise (e.g., an old photograph), we can attempt to restore it based on our probabilistic model of what images look like.  Specifically, we want to generate a graphical model that does a good job at modeling the posterior distribution <script type="math/tex">p(\textsf{original image} \mid \textsf{noisy image}).</script>  Then, by observing the noisy image, we can sample or use exact inference to predict the original image.</p>

<p><img src="imageDenoising4.png" alt="Image Denoising" /></p>

<h2 id="language-models">Language Models</h2>

<p>Knowing the probability distribution can also help us model natural langauge utterances. In this case, we want to construct a probability distribution <script type="math/tex">p(x)</script> over sequences of words or characters <script type="math/tex">x</script> that assigns high probability to proper (English) sentences.  This distribution can be learned from a variety of sources, such as Wikipedia articles.</p>

<p><a id="text-generation"></a></p>
<h3 id="generation">Generation</h3>

<p>Let’s say that we have constructed a distribution of word sequences from Wikipedia articles.  We can then sample from this distribution to generate new Wikipedia-like articles like the one below<label for="1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="1" class="margin-toggle" /><span class="sidenote">From <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> </span>.</p>

<blockquote>
  <p>Naturalism and decision for the majority of Arab countries’ capitalide was grounded
by the Irish language by [[John Clair]], [[An Imperial Japanese Revolt]], associated 
with Guangzham’s sovereignty. His generals were the powerful ruler of the Portugal 
in the [[Protestant Immineners]], which could be said to be directly in Cantonese 
Communication, which followed a ceremony and set inspired prison, training. The 
emperor travelled back to [[Antioch, Perth, October 25|21]] to note, the Kingdom 
of Costa Rica, unsuccessful fashioned the [[Thrales]], [[Cynth’s Dajoard]], known 
in western [[Scotland]], near Italy to the conquest of India with the conflict. 
Copyright was the succession of independence in the slop of Syrian influence that 
was a famous German movement based on a more popular servicious, non-doctrinal 
and sexual power post. Many governments recognize the military housing of the 
[[Civil Liberalization and Infantry Resolution 265 National Party in Hungary]], 
that is sympathetic to be to the [[Punjab Resolution]]
(PJS)[http://www.humah.yahoo.com/guardian.
cfm/7754800786d17551963s89.htm Official economics Adjoint for the Nazism, Montgomery 
was swear to advance to the resources for those Socialism’s rule, 
was starting to signing a major tripad of aid exile.]]</p>
</blockquote>

<p><a id="text-translation"></a></p>
<h3 id="translation">Translation</h3>

<p>Suppose that we have gathered a training set of paragraphs that were transcribed in both English and Chinese. We can build a probabilistic model <script type="math/tex">p(y \mid x)</script> to generate an English sentence <script type="math/tex">y</script> conditioned on the corresponding Chinese sentence <script type="math/tex">x</script>; this is an instance of <em>machine translation</em>.</p>

<p><img src="nmt-model-fast.gif" alt="Neural Machine Translation" /></p>

<h2 id="audio-models">Audio Models</h2>

<p>We can also use probabilitic graphical models for audio applications. Suppose we construct a probability distribution <script type="math/tex">p(x)</script> over audio signals that assigns high probability to ones that sound like human speech.</p>

<p><a id="audio-superresolution"></a></p>
<h3 id="upsampling-or-super-resolution">Upsampling or Super-Resolution</h3>

<p>Given a low resolution version of an audio signal, we can attempt to increase its resolution.  We can formulate this problem as follows: given our speech probability distribution <script type="math/tex">p(x)</script> that “knows” what typical human speech sounds like and some observed values of an audio signal, we aim to calculate signal values at intermediate time points.</p>

<p>In the diagram below, given observed audio signals (blue) and some underlying model of the audio, we aim to reconstruct a higher-fidelity version of the original signal (dotted line) by predicting intermediate signals (white).</p>

<p><img src="audioSuperresolution.png" alt="Audio Super-Resolution" /></p>

<p>We can solve this by sampling or performing inference on <script type="math/tex">p(\textbf{I} \mid \textbf{O})</script>, where <script type="math/tex">\textbf{I}</script> are the intermediate signals that we want to predict, and <script type="math/tex">\textbf{O}</script> are the observed low-resolution audio signals.</p>

<p><a href="https://kuleshov.github.io/audio-super-res/">Super resolution of audio signals demo</a></p>

<p><a id="speech-synthesis"></a></p>
<h3 id="speech-synthesis">Speech synthesis</h3>

<p>As we did in image processing, we can also sample the model and generate (synthesize) speech signals.</p>

<p><a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/">Super resolution of audio signals demo</a></p>

<p><a id="speech-recognition"></a></p>
<h3 id="speech-recognition">Speech recognition</h3>
<p>Given a (joint) model of speech signals and language (text), we can attempt to infer spoken words from audio signals.
<img src="speech.png" alt="Speech" /></p>

<h2 id="applications-in-science-today">Applications in Science Today</h2>

<p><a id="error-correcting-codes"></a></p>
<h3 id="error-correcting-codes">Error Correcting Codes</h3>
<p>In the non-theoretical world, probabilistic models are often used to model communication channels (e.g., Ethernet or Wifi). I.e., if you send a message over a channel, you might get something different on the other end due to noise. Error correcting codes and techniques based on graphical models are used to detect and correct communication errors.
<img src="Picture1.png" alt="codes" /></p>

<p><a id="comp-bio"></a></p>
<h3 id="computational-biology">Computational Biology</h3>

<p>Graphical models are also widely used in computational biology. For example, given a model of how DNA sequences evolve over time, it is possible to reconstruct a phylogenetic tree from DNA sequences of a given set of species.
<img src="philo.png" alt="philo" /></p>

<p><a id="ecology"></a></p>
<h3 id="ecology">Ecology</h3>
<p>Graphical models are used to study phenomena that evolve over space and time, capturing spatial and temporal dependencies. For example, they can be used to study bird migrations.</p>

<p><img src="bird_new.gif" alt="birds" /></p>

<p><a id="economics"></a></p>
<h3 id="economics">Economics</h3>

<p>Graphical models can be used to model spatial distributions of quantities of interests (e.g., assets or expenditures based measures of wealth).
<img src="uganda.png.jpg" alt="birds" /></p>

<p>The last two applications are what are known as spatio-temporal models. They depend on data that is collected across time as well as space.</p>

<p><br /></p>

<table>
  <tbody>
    <tr>
      <td><a href="../../">Index</a></td>
      <td><a href="../probabilityreview/">Previous</a></td>
      <td><a href="../../representation/directed/">Next</a></td>
    </tr>
  </tbody>
</table>



    </article>
    <span class="print-footer">Real-World Applications - Volodymyr Kuleshov</span>
    <footer>
  <hr class="slender">
  <!-- <ul class="footer&#45;links"> -->
  <!--   <li><a href="mailto:hate@spam.net"><span class="icon&#45;mail"></span></a></li>     -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.twitter.com/twitter_handle"><span class="icon-twitter"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//plus.google.com/+googlePlusName"><span class="icon-googleplus"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//github.com/GithubHandle"><span class="icon-github"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="//www.flickr.com/photos/FlickrUserID"><span class="icon-flickr"></span></a> -->
  <!--     </li> -->
  <!--    -->
  <!--     <li> -->
  <!--       <a href="/feed"><span class="icon-feed"></span></a> -->
  <!--     </li> -->
  <!--      -->
  <!-- </ul> -->
<div class="credits">
<!-- <span>&#38;copy; 2018 <!&#45;&#45; &#38;#38;nbsp;&#38;#38;nbsp;VOLODYMYR KULESHOV &#45;&#45;></span></br> <br> -->
<span>Site created with <a href="//jekyllrb.com">Jekyll</a> using the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme</a>. &copy; 2018</span> 
</div>  
</footer>

  </body>
</html>
